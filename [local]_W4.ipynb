{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "31/07/2024 \n",
    "@Lý Linh - AI báo cáo kết quả chạy MinerU. \n",
    "Kiến trúc MinerU gồm 3 model: Layout, MFD, MFR.\n",
    "Detect và OCR => output: 3 file layout.pdf, span.pdf và file mardown. \n",
    "Vấn đề/bán vấn đề:\n",
    "Có 1 đoạn text ở cuối (sau cái bảng dài) không detect được.\n",
    "Bảng chuyển sang ảnh.\n",
    "Chạy bằng CPU 4 trang tầm 10 phút.\n",
    "\n",
    "------------------------------------------\n",
    "=> 3 thành viên chia nhau: clone về test thêm 2-3 pdf khác xem sao? + Xem có báo cái bug đoạn text kia vào nhóm discord của team trên github họ đang làm không.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tool AI: https://chromewebstore.google.com/detail/harpa-ai-automation-agent/eanggfilgoajaocelnaflolkadkeghjp\n",
    "\n",
    "0. Conda\n",
    "```\n",
    "conda create -n MinerU python=3.10\n",
    "conda activate MinerU\n",
    "```\n",
    "\n",
    "1. Install Magic-PDF: https://pypi.org/project/magic-pdf/\n",
    "```\n",
    "pip install magic-pdf\n",
    "```\n",
    "The high-precision models depend on detectron2\n",
    "```\n",
    "pip install detectron2 --extra-index-url https://myhloli.github.io/wheels/\n",
    "```\n",
    "\n",
    "2. Downloading model weights files: \n",
    "```\n",
    "git lfs clone https://huggingface.co/wanderkid/PDF-Extract-Kit\n",
    "```\n",
    "\n",
    "Mở ra ta thấy đủ \n",
    "```\n",
    "../\n",
    "├── Layout\n",
    "│   ├── config.json\n",
    "│   └── model_final.pth\n",
    "├── MFD\n",
    "│   └── weights.pt\n",
    "├── MFR\n",
    "│   └── UniMERNet\n",
    "│       ├── config.json\n",
    "│       ├── preprocessor_config.json\n",
    "│       ├── pytorch_model.bin\n",
    "│       ├── README.md\n",
    "│       ├── tokenizer_config.json\n",
    "│       └── tokenizer.json\n",
    "│── TabRec\n",
    "│   └─StructEqTable\n",
    "│       ├── config.json\n",
    "│       ├── generation_config.json\n",
    "│       ├── model.safetensors\n",
    "│       ├── preprocessor_config.json\n",
    "│       ├── special_tokens_map.json\n",
    "│       ├── spiece.model\n",
    "│       ├── tokenizer.json\n",
    "│       └── tokenizer_config.json \n",
    "└── README.md\n",
    "```\n",
    "\n",
    "3. \n",
    "Git clone \n",
    "```\n",
    "https://github.com/opendatalab/MinerU\n",
    "```\n",
    "\n",
    "Find: find the `magic-pdf.template.json` file template configuration file in the root directory of the repository.\n",
    "để thay đường dẫn của `model weights files` vào nha và đổi tên mới `magic-pdf.json`\n",
    "\n",
    "4. Usage\n",
    "\n",
    "`magic-pdf pdf-command --pdf \"pdf_path\" --model \"model_json_path\"`\n",
    "\n",
    "Run \n",
    "```\n",
    "magic-pdf pdf-command --pdf \"D:\\OneDrive - Hanoi University of Science and Technology\\ITE10-DS&AI-HUST\\Learn&Task\\AI_Machine_Deep\\AI-ML-DL\\20232 NLP - M8 AIO\\2024_PDF_Parser_CODEMELY\\pdf_test\\Test1page.pdf\" --model \"D:\\OneDrive - Hanoi University of Science and Technology\\ITE10-DS&AI-HUST\\Learn&Task\\AI_Machine_Deep\\AI-ML-DL\\20232 NLP - M8 AIO\\2024_PDF_Parser_CODEMELY\\MinerU\\magic-pdf.json\"\n",
    "```\n",
    "\n",
    "Bug: `Exception: Both parse_pdf_by_txt and parse_pdf_by_ocr failed.`\n",
    "```\n",
    "Fix bug: \n",
    "- Chú ý file `magic-pdf.json`\n",
    "- Update: `pip install magic-pdf[full]==0.7.0b1 --extra-index-url https://wheels.myhloli.com`\n",
    "- Update help: `cd MinerU` -> \n",
    "\n",
    "`\n",
    "magic-pdf --path \"D:\\OneDrive - Hanoi University of Science and Technology\\ITE10-DS&AI-HUST\\Learn&Task\\AI_Machine_Deep\\AI-ML-DL\\20232 NLP - M8 AIO\\2024_PDF_Parser_CODEMELY\\pdf_test\\Test1page.pdf\" --method \"auto\"\n",
    "`\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phân tích Output: \n",
    "```\n",
    "[08/18 16:07:35 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/PDF-Extract-Kit/models\\Layout/model_final.pth ...\n",
    "[08/18 16:07:35 fvcore.common.checkpoint]: [Checkpointer] Loading from d:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/PDF-Extract-Kit/models\\Layout/model_final.pth ...\n",
    "download https://paddleocr.bj.bcebos.com/PP-OCRv4/chinese/ch_PP-OCRv4_det_infer.tar to C:\\Users\\User/.paddleocr/whl\\det\\ch\\ch_PP-OCRv4_det_infer\\ch_PP-OCRv4_det_infer.tar\n",
    "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4.89M/4.89M [00:08<00:00, 567kiB/s]\n",
    "download https://paddleocr.bj.bcebos.com/PP-OCRv4/chinese/ch_PP-OCRv4_rec_infer.tar to C:\\Users\\User/.paddleocr/whl\\rec\\ch\\ch_PP-OCRv4_rec_infer\\ch_PP-OCRv4_rec_infer.tar\n",
    "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11.0M/11.0M [00:09<00:00, 1.12MiB/s]\n",
    "download https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar to C:\\Users\\User/.paddleocr/whl\\cls\\ch_ppocr_mobile_v2.0_cls_infer\\ch_ppocr_mobile_v2.0_cls_infer.tar\n",
    "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2.19M/2.19M [00:04<00:00, 529kiB/s]\n",
    "```\n",
    "\n",
    "- Detectron2: Framework Detectron2 được khởi tạo trong quá trình xử lý. Đây là một framework mạnh mẽ được phát triển bởi Facebook AI Research (FAIR), thường được sử dụng cho các tác vụ như phát hiện đối tượng, phân loại hình ảnh, và nhận diện layout trong tài liệu.\n",
    "\n",
    "- Model config file: Output đã chỉ ra rằng file cấu hình được sử dụng là layoutlmv3_base_inference.yaml, được liên kết với mô hình LayoutLMv3, một mô hình đặc biệt được thiết kế để hiểu và xử lý layout của văn bản trong tài liệu.\n",
    "\n",
    "Dựa trên log bạn cung cấp, có thể thấy rằng hệ thống đang sử dụng các model từ **PaddleOCR**, cụ thể là:\n",
    "\n",
    "1. **ch_PP-OCRv4_det_infer**: Đây là mô hình phát hiện chữ viết (text detection) của PaddleOCR, phiên bản v4 dành cho tiếng Trung Quốc. Model này được tải về từ PaddleOCR và được sử dụng để xác định vị trí của văn bản trong ảnh.\n",
    "\n",
    "2. **ch_PP-OCRv4_rec_infer**: Đây là mô hình nhận dạng chữ viết (text recognition) của PaddleOCR, cũng là phiên bản v4 dành cho tiếng Trung Quốc. Model này được tải về và sử dụng để chuyển đổi hình ảnh chứa văn bản đã phát hiện thành dạng ký tự văn bản.\n",
    "\n",
    "3. **ch_ppocr_mobile_v2.0_cls_infer**: Đây là mô hình phân loại định hướng của văn bản (text direction classification) của PaddleOCR, phiên bản v2.0. Nó giúp xác định và điều chỉnh hướng của văn bản trước khi đưa vào các bước xử lý OCR tiếp theo.\n",
    "\n",
    "Custom Models: Có một số models custom được khởi tạo, như CustomVisionEncoderDecoderModel, CustomMBartForCausalLM, và CustomMBartDecoder, cho thấy rằng hệ thống đang sử dụng các mô hình tùy chỉnh cho nhiều bước trong pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\dict2md\\ocr_mkcontent.py, Line 5: from magic_pdf.libs.language import detect_lang\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\dict2md\\ocr_mkcontent.py, Line 66: language = ''\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\dict2md\\ocr_mkcontent.py, Line 69: language = detect_lang(content)\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\dict2md\\ocr_mkcontent.py, Line 70: if language == 'en':  # 只对英文长词进行分词处理，中文分词会丢失文本\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\dict2md\\ocr_mkcontent.py, Line 84: if language == 'en':  # 英文语境下 content间需要空格分隔\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\dict2md\\ocr_mkcontent.py, Line 153: line_lang = \"\"\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\dict2md\\ocr_mkcontent.py, Line 159: line_lang = detect_lang(line_text)\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\dict2md\\ocr_mkcontent.py, Line 165: language = detect_lang(content)\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\dict2md\\ocr_mkcontent.py, Line 166: if language == 'en':  # 只对英文长词进行分词处理，中文分词会丢失文本\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\dict2md\\ocr_mkcontent.py, Line 176: if 'zh' in line_lang:  # 遇到一些一个字一个span的文档，这种单字语言判断不准，需要用整行文本判断\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\dict2md\\ocr_mkcontent.py, Line 192: language = ''\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\dict2md\\ocr_mkcontent.py, Line 197: language = detect_lang(content)\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\dict2md\\ocr_mkcontent.py, Line 198: if language == 'en':  # 只对英文长词进行分词处理，中文分词会丢失文本\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\dict2md\\ocr_mkcontent.py, Line 206: if language == 'en':  # 英文语境下 content间需要空格分隔\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\filter\\pdf_meta_scan.py, Line 14: from magic_pdf.libs.language import detect_lang\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\filter\\pdf_meta_scan.py, Line 275: def get_language(doc: fitz.Document):\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\filter\\pdf_meta_scan.py, Line 283: language_lst = []\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\filter\\pdf_meta_scan.py, Line 289: page_language = detect_lang(text_block)\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\filter\\pdf_meta_scan.py, Line 290: language_lst.append(page_language)\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\filter\\pdf_meta_scan.py, Line 292: # logger.info(f\"page_id: {page_id}, page_language: {page_language}\")\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\filter\\pdf_meta_scan.py, Line 294: # 统计text_language_list中每种语言的个数\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\filter\\pdf_meta_scan.py, Line 295: count_dict = Counter(language_lst)\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\filter\\pdf_meta_scan.py, Line 296: # 输出text_language_list中出现的次数最多的语言\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\filter\\pdf_meta_scan.py, Line 297: language = max(count_dict, key=count_dict.get)\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\filter\\pdf_meta_scan.py, Line 298: return language\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\filter\\pdf_meta_scan.py, Line 337: text_language = get_language(doc)\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\filter\\pdf_meta_scan.py, Line 338: # logger.info(f\"text_language: {text_language}\")\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\filter\\pdf_meta_scan.py, Line 352: \"text_language\": text_language,\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\commons.py, Line 201: s3_profile = \"langchao\"\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\detect_language_from_model.py, Line 3: from magic_pdf.libs.language import detect_lang\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\detect_language_from_model.py, Line 5: def get_language_from_model(model_list: list):\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\detect_language_from_model.py, Line 6: language_lst = []\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\detect_language_from_model.py, Line 15: page_language = detect_lang(page_text)\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\detect_language_from_model.py, Line 16: language_lst.append(page_language)\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\detect_language_from_model.py, Line 17: # 统计text_language_list中每种语言的个数\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\detect_language_from_model.py, Line 18: count_dict = Counter(language_lst)\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\detect_language_from_model.py, Line 19: # 输出text_language_list中出现的次数最多的语言\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\detect_language_from_model.py, Line 20: language = max(count_dict, key=count_dict.get)\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\detect_language_from_model.py, Line 21: return language\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\drop_reason.py, Line 21: NOT_ALLOW_LANGUAGE = \"not_allow_language\" # 不支持的语种\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\language.py, Line 8: ftlang_cache_dir = os.path.join(root_dir, 'resources', 'fasttext-langdetect')\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\language.py, Line 9: os.environ[\"FTLANG_CACHE\"] = str(ftlang_cache_dir)\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\language.py, Line 12: from fast_langdetect import detect_language\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\language.py, Line 15: def detect_lang(text: str) -> str:\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\language.py, Line 20: lang_upper = detect_language(text)\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\language.py, Line 23: lang_upper = detect_language(html_no_ctrl_chars)\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\language.py, Line 25: lang = lang_upper.lower()\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\language.py, Line 27: lang = \"\"\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\language.py, Line 28: return lang\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\language.py, Line 33: print(detect_lang(\"This is a test.\"))\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\language.py, Line 34: print(detect_lang(\"<html>This is a test</html>\"))\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\language.py, Line 35: print(detect_lang(\"这个是中文测试。\"))\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\language.py, Line 36: print(detect_lang(\"<html>这个是中文测试。</html>\"))\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\nlp_utils.py, Line 8: # from langdetect import detect\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\nlp_utils.py, Line 13: from magic_pdf.libs.language import detect_lang\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\nlp_utils.py, Line 82: def detect_language(self, text, use_langdetect=False):\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\nlp_utils.py, Line 85: if use_langdetect:\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\nlp_utils.py, Line 86: # print(\"use_langdetect\")\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\nlp_utils.py, Line 87: # print(detect_lang(text))\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\nlp_utils.py, Line 88: # return detect_lang(text)\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\nlp_utils.py, Line 89: if detect_lang(text) == \"zh\":\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\nlp_utils.py, Line 94: if not use_langdetect:\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\nlp_utils.py, Line 118: lang = self.detect_language(text, use_langdetect=True)\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\nlp_utils.py, Line 120: if lang == \"en\":\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\nlp_utils.py, Line 122: elif lang == \"zh\":\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\nlp_utils.py, Line 125: # logger.error(f\"Unsupported language: {lang}\")\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\model\\pek_sub_modules\\layoutlmv3\\layoutlmft\\data\\xfund.py, Line 155: self.cur_la = args.language\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\model\\pek_sub_modules\\layoutlmv3\\layoutlmft\\models\\layoutlmv3\\modeling_layoutlmv3.py, Line 14: # See the License for the specific language governing permissions and\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\model\\pek_sub_modules\\layoutlmv3\\layoutlmft\\models\\layoutlmv3\\tokenization_layoutlmv3.py, Line 13: # See the License for the specific language governing permissions and\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split.py, Line 23: def __detect_list_lines(lines, new_layout_bboxes, lang):\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split.py, Line 72: if lang!='en':\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split.py, Line 229: def __group_line_by_layout(blocks, layout_bboxes, lang=\"en\"):\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split.py, Line 243: def __split_para_in_layoutbox(lines_group, new_layout_bbox, lang=\"en\", char_avg_len=10):\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split.py, Line 275: text_segments, list_start_line = __detect_list_lines(lines, new_layout_bbox, lang)\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split.py, Line 337: def __connect_list_inter_layout(layout_paras, new_layout_bbox, layout_list_info, page_num, lang):\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split.py, Line 373: def __connect_list_inter_page(pre_page_paras, next_page_paras, pre_page_layout_bbox, next_page_layout_bbox,  pre_page_list_info, next_page_list_info, page_num, lang):\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split.py, Line 413: def __connect_para_inter_layoutbox(layout_paras, new_layout_bbox, lang):\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split.py, Line 464: def __connect_para_inter_page(pre_page_paras, next_page_paras, pre_page_layout_bbox, next_page_layout_bbox, page_num, lang):\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split.py, Line 523: def __connect_middle_align_text(page_paras, new_layout_bbox, page_num, lang, debug_mode):\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split.py, Line 570: def __merge_signle_list_text(page_paras, new_layout_bbox, page_num, lang):\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split.py, Line 578: def __do_split_page(blocks, layout_bboxes, new_layout_bbox, page_num, lang):\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split.py, Line 592: lines_group = __group_line_by_layout(blocks, layout_bboxes, lang) # block内分段\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split.py, Line 593: layout_paras, layout_list_info = __split_para_in_layoutbox(lines_group, new_layout_bbox, lang) # layout内分段\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split.py, Line 594: layout_paras2, page_list_info = __connect_list_inter_layout(layout_paras, new_layout_bbox, layout_list_info, page_num, lang) # layout之间连接列表段落\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split.py, Line 595: connected_layout_paras = __connect_para_inter_layoutbox(layout_paras2, new_layout_bbox, lang) # layout间链接段落\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split.py, Line 601: def para_split(pdf_info_dict, debug_mode, lang=\"en\"):\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split.py, Line 612: splited_blocks, page_list_info = __do_split_page(blocks, layout_bboxes, new_layout_bbox, page_num, lang)\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split.py, Line 626: is_conn = __connect_para_inter_page(pre_page_paras, next_page_paras, pre_page_layout_bbox, next_page_layout_bbox, page_num, lang)\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split.py, Line 631: is_list_conn = __connect_list_inter_page(pre_page_paras, next_page_paras, pre_page_layout_bbox, next_page_layout_bbox, all_page_list_info[page_num-1], all_page_list_info[page_num], page_num, lang)\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split.py, Line 643: __connect_middle_align_text(page_paras, new_layout_bbox, page_num, lang, debug_mode=debug_mode)\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split.py, Line 644: __merge_signle_list_text(page_paras, new_layout_bbox, page_num, lang)\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split_v2.py, Line 25: def __detect_list_lines(lines, new_layout_bboxes, lang):\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split_v2.py, Line 101: if lang != 'en':\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split_v2.py, Line 317: def __split_para_in_layoutbox(blocks_group, new_layout_bbox, lang=\"en\"):\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split_v2.py, Line 356: text_segments, list_start_line = __detect_list_lines(lines, new_layout_bbox, lang)\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split_v2.py, Line 411: def __connect_list_inter_layout(blocks_group, new_layout_bbox, layout_list_info, page_num, lang):\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split_v2.py, Line 451: pre_page_list_info, next_page_list_info, page_num, lang):\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split_v2.py, Line 570: lang):\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split_v2.py, Line 653: def __connect_middle_align_text(page_paras, new_layout_bbox, page_num, lang):\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split_v2.py, Line 707: def __merge_signle_list_text(page_paras, new_layout_bbox, page_num, lang):\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split_v2.py, Line 715: def __do_split_page(blocks, layout_bboxes, new_layout_bbox, page_num, lang):\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split_v2.py, Line 728: layout_list_info = __split_para_in_layoutbox(blocks_group, new_layout_bbox, lang)  # layout内分段\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split_v2.py, Line 730: page_num, lang)  # layout之间连接列表段落\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split_v2.py, Line 736: def para_split(pdf_info_dict, debug_mode, lang=\"en\"):\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split_v2.py, Line 746: splited_blocks, page_list_info = __do_split_page(blocks, layout_bboxes, new_layout_bbox, page_num, lang)\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split_v2.py, Line 761: next_page_layout_bbox, page_num, lang)\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split_v2.py, Line 768: all_page_list_info[page_num], page_num, lang)\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split_v2.py, Line 780: __connect_middle_align_text(page_paras, new_layout_bbox, page_num, lang)\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split_v2.py, Line 781: __merge_signle_list_text(page_paras, new_layout_bbox, page_num, lang)\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pdf_text_example\\vertical_en_blocks.json, Line 451: \"text\": \"language downstream tasks. Current approaches\",\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pdf_text_example\\vertical_en_blocks.json, Line 1310: \"text\": \"joint domain of vision and language, giving birth to the cat-\",\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pdf_text_example\\vertical_en_blocks.json, Line 5453: \"text\": \"with image text matching and masked language modeling\",\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pdf_text_example\\vertical_en_blocks.json, Line 5578: \"text\": \"\\ufb01ne-tuned on vision-and-language downstream tasks where\",\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pdf_text_example\\vertical_en_blocks.json, Line 5692: \"text\": \"tially embedded in a dense form alongside language tokens.\",\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pre_proc_results\\2列_ViLT_1_title.pdf\\preproc_out.json, Line 454: \"text\": \"language downstream tasks. Current approaches\",\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pre_proc_results\\2列_ViLT_1_title.pdf\\preproc_out.json, Line 1313: \"text\": \"joint domain of vision and language, giving birth to the cat-\",\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pre_proc_results\\2列_ViLT_1_title.pdf\\preproc_out.json, Line 2957: \"text\": \"with image text matching and masked language modeling\",\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pre_proc_results\\2列_ViLT_1_title.pdf\\preproc_out.json, Line 3082: \"text\": \"ﬁne-tuned on vision-and-language downstream tasks where\",\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pre_proc_results\\2列_ViLT_1_title.pdf\\preproc_out.json, Line 3196: \"text\": \"tially embedded in a dense form alongside language tokens.\",\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pre_proc_results\\2列_ViLT_1_title.pdf\\preproc_out.json, Line 4162: \"text\": \"language downstream tasks. Current approaches\",\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pre_proc_results\\2列_ViLT_1_title.pdf\\preproc_out.json, Line 5021: \"text\": \"joint domain of vision and language, giving birth to the cat-\",\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pre_proc_results\\2列_ViLT_1_title.pdf\\preproc_out.json, Line 9164: \"text\": \"with image text matching and masked language modeling\",\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pre_proc_results\\2列_ViLT_1_title.pdf\\preproc_out.json, Line 9289: \"text\": \"ﬁne-tuned on vision-and-language downstream tasks where\",\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pre_proc_results\\2列_ViLT_1_title.pdf\\preproc_out.json, Line 9403: \"text\": \"tially embedded in a dense form alongside language tokens.\",\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pre_proc_results\\纯2列_10.1002_mrm.24141_4.pdf\\preproc_out.json, Line 1394: \"text\": \"standard 1.5T clinical MRI scanner (Siemens, Erlangen,\",\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pre_proc_results\\纯2列_10.1002_mrm.24141_4.pdf\\preproc_out.json, Line 7126: \"text\": \"standard 1.5T clinical MRI scanner (Siemens, Erlangen,\",\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pre_proc_results\\纯2列_ViLT_6_文字_表格.pdf\\preproc_out.json, Line 199: \"text\": \"and-language downstream tasks: for\",\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pre_proc_results\\纯2列_ViLT_6_文字_表格.pdf\\preproc_out.json, Line 2501: \"text\": \"performance gain for vision and language downstream tasks.\",\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pre_proc_results\\纯2列_ViLT_6_文字_表格.pdf\\preproc_out.json, Line 3128: \"text\": \"language. The annotated answers are originally in free-form\",\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pre_proc_results\\纯2列_ViLT_6_文字_表格.pdf\\preproc_out.json, Line 3162: \"text\": \"natural language, but it is a common practice to convert the\",\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pre_proc_results\\纯2列_ViLT_6_文字_表格.pdf\\preproc_out.json, Line 5068: \"text\": \"and-language downstream tasks: for\",\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pre_proc_results\\纯2列_ViLT_6_文字_表格.pdf\\preproc_out.json, Line 7370: \"text\": \"performance gain for vision and language downstream tasks.\",\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pre_proc_results\\纯2列_ViLT_6_文字_表格.pdf\\preproc_out.json, Line 11423: \"text\": \"language. The annotated answers are originally in free-form\",\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pre_proc_results\\纯2列_ViLT_6_文字_表格.pdf\\preproc_out.json, Line 11457: \"text\": \"natural language, but it is a common practice to convert the\",\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/PDF-Extract-Kit\\models\\MFR\\UniMERNet\\tokenizer.json, Line 2976: \"langle\": 2634,\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/PDF-Extract-Kit\\models\\MFR\\UniMERNet\\tokenizer.json, Line 4456: \"Ġlanguage\": 4114,\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/PDF-Extract-Kit\\models\\MFR\\UniMERNet\\tokenizer.json, Line 10084: \"Ġlanguages\": 9742,\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/PDF-Extract-Kit\\models\\MFR\\UniMERNet\\tokenizer.json, Line 22360: \"lang\": 22018,\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/PDF-Extract-Kit\\models\\MFR\\UniMERNet\\tokenizer.json, Line 22948: \"language\": 22606,\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/PDF-Extract-Kit\\models\\MFR\\UniMERNet\\tokenizer.json, Line 32791: \"Ġcholangi\": 32449,\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/PDF-Extract-Kit\\models\\MFR\\UniMERNet\\tokenizer.json, Line 46236: \"ĠErlang\": 45894,\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/PDF-Extract-Kit\\models\\MFR\\UniMERNet\\tokenizer.json, Line 47351: \"Ġlang\": 47009,\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/PDF-Extract-Kit\\models\\MFR\\UniMERNet\\tokenizer.json, Line 47614: \"Ġcholangiocarcinoma\": 47272,\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/PDF-Extract-Kit\\models\\MFR\\UniMERNet\\tokenizer.json, Line 48483: \"Ġcholang\": 48141,\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/PDF-Extract-Kit\\models\\MFR\\UniMERNet\\tokenizer.json, Line 95959: \"ĠEr lang\",\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/PDF-Extract-Kit\\models\\MFR\\UniMERNet\\tokenizer.json, Line 97337: \"Ġcholangi ocarcinoma\",\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/PDF-Extract-Kit\\models\\TabRec\\StructEqTable\\tokenizer.json, Line 5538: \"▁language\",\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/PDF-Extract-Kit\\models\\TabRec\\StructEqTable\\tokenizer.json, Line 12038: \"lang\",\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/PDF-Extract-Kit\\models\\TabRec\\StructEqTable\\tokenizer.json, Line 19094: \"▁languages\",\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/PDF-Extract-Kit\\models\\TabRec\\StructEqTable\\tokenizer.json, Line 34678: \"language\",\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/PDF-Extract-Kit\\models\\TabRec\\StructEqTable\\tokenizer.json, Line 47918: \"▁lang\",\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/PDF-Extract-Kit\\models\\TabRec\\StructEqTable\\tokenizer.json, Line 72542: \"Alang\",\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/PDF-Extract-Kit\\models\\TabRec\\StructEqTable\\tokenizer.json, Line 105450: \"▁slang\",\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/PDF-Extract-Kit\\models\\TabRec\\StructEqTable\\tokenizer.json, Line 153270: \"▁lange\",\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/PDF-Extract-Kit\\models\\TabRec\\StructEqTable\\tokenizer.json, Line 159854: \"▁langue\",\n",
      "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/PDF-Extract-Kit\\models\\TabRec\\StructEqTable\\tokenizer.json, Line 163446: \"languages\",\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def find_keywords_in_files(directory, keywords):\n",
    "    found = False  # Biến để theo dõi xem có tìm thấy từ khóa hay không\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".py\") or file.endswith(\".yaml\") or file.endswith(\".json\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        content = f.readlines()\n",
    "                        for i, line in enumerate(content):\n",
    "                            if any(keyword in line for keyword in keywords):\n",
    "                                print(f\"File: {file_path}, Line {i+1}: {line.strip()}\")\n",
    "                                found = True  # Đánh dấu đã tìm thấy từ khóa\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not read file {file_path}: {e}\")\n",
    "\n",
    "    if not found:\n",
    "        print(\"Không có từ khóa nào được tìm thấy\")\n",
    "\n",
    "# Đường dẫn đến thư mục gốc của dự án\n",
    "directory = \"D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/\"\n",
    "\n",
    "# Các từ khóa cần tìm\n",
    "keywords = [\n",
    "    \"lang\",\n",
    "]\n",
    "\n",
    "find_keywords_in_files(directory, keywords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\dict2md\\ocr_mkcontent.py, Line 5: from magic_pdf.libs.language import detect_lang\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\dict2md\\ocr_mkcontent.py, Line 66: language = ''\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\dict2md\\ocr_mkcontent.py, Line 69: language = detect_lang(content)\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\dict2md\\ocr_mkcontent.py, Line 70: if language == 'en':  # 只对英文长词进行分词处理，中文分词会丢失文本\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\dict2md\\ocr_mkcontent.py, Line 84: if language == 'en':  # 英文语境下 content间需要空格分隔\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\dict2md\\ocr_mkcontent.py, Line 153: line_lang = \"\"\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\dict2md\\ocr_mkcontent.py, Line 159: line_lang = detect_lang(line_text)\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\dict2md\\ocr_mkcontent.py, Line 165: language = detect_lang(content)\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\dict2md\\ocr_mkcontent.py, Line 166: if language == 'en':  # 只对英文长词进行分词处理，中文分词会丢失文本\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\dict2md\\ocr_mkcontent.py, Line 176: if 'zh' in line_lang:  # 遇到一些一个字一个span的文档，这种单字语言判断不准，需要用整行文本判断\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\dict2md\\ocr_mkcontent.py, Line 192: language = ''\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\dict2md\\ocr_mkcontent.py, Line 197: language = detect_lang(content)\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\dict2md\\ocr_mkcontent.py, Line 198: if language == 'en':  # 只对英文长词进行分词处理，中文分词会丢失文本\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\dict2md\\ocr_mkcontent.py, Line 206: if language == 'en':  # 英文语境下 content间需要空格分隔\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\filter\\pdf_meta_scan.py, Line 14: from magic_pdf.libs.language import detect_lang\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\filter\\pdf_meta_scan.py, Line 275: def get_language(doc: fitz.Document):\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\filter\\pdf_meta_scan.py, Line 283: language_lst = []\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\filter\\pdf_meta_scan.py, Line 289: page_language = detect_lang(text_block)\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\filter\\pdf_meta_scan.py, Line 290: language_lst.append(page_language)\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\filter\\pdf_meta_scan.py, Line 292: # logger.info(f\"page_id: {page_id}, page_language: {page_language}\")\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\filter\\pdf_meta_scan.py, Line 294: # 统计text_language_list中每种语言的个数\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\filter\\pdf_meta_scan.py, Line 295: count_dict = Counter(language_lst)\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\filter\\pdf_meta_scan.py, Line 296: # 输出text_language_list中出现的次数最多的语言\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\filter\\pdf_meta_scan.py, Line 297: language = max(count_dict, key=count_dict.get)\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\filter\\pdf_meta_scan.py, Line 298: return language\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\filter\\pdf_meta_scan.py, Line 337: text_language = get_language(doc)\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\filter\\pdf_meta_scan.py, Line 338: # logger.info(f\"text_language: {text_language}\")\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\filter\\pdf_meta_scan.py, Line 352: \"text_language\": text_language,\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\commons.py, Line 201: s3_profile = \"langchao\"\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\detect_language_from_model.py, Line 3: from magic_pdf.libs.language import detect_lang\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\detect_language_from_model.py, Line 5: def get_language_from_model(model_list: list):\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\detect_language_from_model.py, Line 6: language_lst = []\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\detect_language_from_model.py, Line 15: page_language = detect_lang(page_text)\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\detect_language_from_model.py, Line 16: language_lst.append(page_language)\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\detect_language_from_model.py, Line 17: # 统计text_language_list中每种语言的个数\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\detect_language_from_model.py, Line 18: count_dict = Counter(language_lst)\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\detect_language_from_model.py, Line 19: # 输出text_language_list中出现的次数最多的语言\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\detect_language_from_model.py, Line 20: language = max(count_dict, key=count_dict.get)\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\detect_language_from_model.py, Line 21: return language\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\drop_reason.py, Line 21: NOT_ALLOW_LANGUAGE = \"not_allow_language\" # 不支持的语种\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\language.py, Line 8: ftlang_cache_dir = os.path.join(root_dir, 'resources', 'fasttext-langdetect')\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\language.py, Line 9: os.environ[\"FTLANG_CACHE\"] = str(ftlang_cache_dir)\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\language.py, Line 12: from fast_langdetect import detect_language\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\language.py, Line 15: def detect_lang(text: str) -> str:\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\language.py, Line 20: lang_upper = detect_language(text)\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\language.py, Line 23: lang_upper = detect_language(html_no_ctrl_chars)\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\language.py, Line 25: lang = lang_upper.lower()\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\language.py, Line 27: lang = \"\"\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\language.py, Line 28: return lang\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\language.py, Line 33: print(detect_lang(\"This is a test.\"))\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\language.py, Line 34: print(detect_lang(\"<html>This is a test</html>\"))\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\language.py, Line 35: print(detect_lang(\"这个是中文测试。\"))\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\language.py, Line 36: print(detect_lang(\"<html>这个是中文测试。</html>\"))\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\nlp_utils.py, Line 8: # from langdetect import detect\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\nlp_utils.py, Line 13: from magic_pdf.libs.language import detect_lang\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\nlp_utils.py, Line 82: def detect_language(self, text, use_langdetect=False):\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\nlp_utils.py, Line 85: if use_langdetect:\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\nlp_utils.py, Line 86: # print(\"use_langdetect\")\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\nlp_utils.py, Line 87: # print(detect_lang(text))\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\nlp_utils.py, Line 88: # return detect_lang(text)\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\nlp_utils.py, Line 89: if detect_lang(text) == \"zh\":\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\nlp_utils.py, Line 94: if not use_langdetect:\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\nlp_utils.py, Line 118: lang = self.detect_language(text, use_langdetect=True)\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\nlp_utils.py, Line 120: if lang == \"en\":\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\nlp_utils.py, Line 122: elif lang == \"zh\":\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\libs\\nlp_utils.py, Line 125: # logger.error(f\"Unsupported language: {lang}\")\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\model\\pek_sub_modules\\layoutlmv3\\layoutlmft\\data\\xfund.py, Line 155: self.cur_la = args.language\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\model\\pek_sub_modules\\layoutlmv3\\layoutlmft\\models\\layoutlmv3\\modeling_layoutlmv3.py, Line 14: # See the License for the specific language governing permissions and\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\model\\pek_sub_modules\\layoutlmv3\\layoutlmft\\models\\layoutlmv3\\tokenization_layoutlmv3.py, Line 13: # See the License for the specific language governing permissions and\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split.py, Line 23: def __detect_list_lines(lines, new_layout_bboxes, lang):\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split.py, Line 72: if lang!='en':\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split.py, Line 229: def __group_line_by_layout(blocks, layout_bboxes, lang=\"en\"):\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split.py, Line 243: def __split_para_in_layoutbox(lines_group, new_layout_bbox, lang=\"en\", char_avg_len=10):\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split.py, Line 275: text_segments, list_start_line = __detect_list_lines(lines, new_layout_bbox, lang)\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split.py, Line 337: def __connect_list_inter_layout(layout_paras, new_layout_bbox, layout_list_info, page_num, lang):\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split.py, Line 373: def __connect_list_inter_page(pre_page_paras, next_page_paras, pre_page_layout_bbox, next_page_layout_bbox,  pre_page_list_info, next_page_list_info, page_num, lang):\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split.py, Line 413: def __connect_para_inter_layoutbox(layout_paras, new_layout_bbox, lang):\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split.py, Line 464: def __connect_para_inter_page(pre_page_paras, next_page_paras, pre_page_layout_bbox, next_page_layout_bbox, page_num, lang):\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split.py, Line 523: def __connect_middle_align_text(page_paras, new_layout_bbox, page_num, lang, debug_mode):\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split.py, Line 570: def __merge_signle_list_text(page_paras, new_layout_bbox, page_num, lang):\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split.py, Line 578: def __do_split_page(blocks, layout_bboxes, new_layout_bbox, page_num, lang):\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split.py, Line 592: lines_group = __group_line_by_layout(blocks, layout_bboxes, lang) # block内分段\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split.py, Line 593: layout_paras, layout_list_info = __split_para_in_layoutbox(lines_group, new_layout_bbox, lang) # layout内分段\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split.py, Line 594: layout_paras2, page_list_info = __connect_list_inter_layout(layout_paras, new_layout_bbox, layout_list_info, page_num, lang) # layout之间连接列表段落\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split.py, Line 595: connected_layout_paras = __connect_para_inter_layoutbox(layout_paras2, new_layout_bbox, lang) # layout间链接段落\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split.py, Line 601: def para_split(pdf_info_dict, debug_mode, lang=\"en\"):\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split.py, Line 612: splited_blocks, page_list_info = __do_split_page(blocks, layout_bboxes, new_layout_bbox, page_num, lang)\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split.py, Line 626: is_conn = __connect_para_inter_page(pre_page_paras, next_page_paras, pre_page_layout_bbox, next_page_layout_bbox, page_num, lang)\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split.py, Line 631: is_list_conn = __connect_list_inter_page(pre_page_paras, next_page_paras, pre_page_layout_bbox, next_page_layout_bbox, all_page_list_info[page_num-1], all_page_list_info[page_num], page_num, lang)\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split.py, Line 643: __connect_middle_align_text(page_paras, new_layout_bbox, page_num, lang, debug_mode=debug_mode)\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split.py, Line 644: __merge_signle_list_text(page_paras, new_layout_bbox, page_num, lang)\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split_v2.py, Line 25: def __detect_list_lines(lines, new_layout_bboxes, lang):\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split_v2.py, Line 101: if lang != 'en':\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split_v2.py, Line 317: def __split_para_in_layoutbox(blocks_group, new_layout_bbox, lang=\"en\"):\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split_v2.py, Line 356: text_segments, list_start_line = __detect_list_lines(lines, new_layout_bbox, lang)\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split_v2.py, Line 411: def __connect_list_inter_layout(blocks_group, new_layout_bbox, layout_list_info, page_num, lang):\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split_v2.py, Line 451: pre_page_list_info, next_page_list_info, page_num, lang):\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split_v2.py, Line 570: lang):\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split_v2.py, Line 653: def __connect_middle_align_text(page_paras, new_layout_bbox, page_num, lang):\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split_v2.py, Line 707: def __merge_signle_list_text(page_paras, new_layout_bbox, page_num, lang):\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split_v2.py, Line 715: def __do_split_page(blocks, layout_bboxes, new_layout_bbox, page_num, lang):\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split_v2.py, Line 728: layout_list_info = __split_para_in_layoutbox(blocks_group, new_layout_bbox, lang)  # layout内分段\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split_v2.py, Line 730: page_num, lang)  # layout之间连接列表段落\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split_v2.py, Line 736: def para_split(pdf_info_dict, debug_mode, lang=\"en\"):\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split_v2.py, Line 746: splited_blocks, page_list_info = __do_split_page(blocks, layout_bboxes, new_layout_bbox, page_num, lang)\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split_v2.py, Line 761: next_page_layout_bbox, page_num, lang)\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split_v2.py, Line 768: all_page_list_info[page_num], page_num, lang)\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split_v2.py, Line 780: __connect_middle_align_text(page_paras, new_layout_bbox, page_num, lang)\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\magic_pdf\\para\\para_split_v2.py, Line 781: __merge_signle_list_text(page_paras, new_layout_bbox, page_num, lang)\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pdf_text_example\\vertical_en_blocks.json, Line 451: \"text\": \"language downstream tasks. Current approaches\",\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pdf_text_example\\vertical_en_blocks.json, Line 1310: \"text\": \"joint domain of vision and language, giving birth to the cat-\",\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pdf_text_example\\vertical_en_blocks.json, Line 5453: \"text\": \"with image text matching and masked language modeling\",\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pdf_text_example\\vertical_en_blocks.json, Line 5578: \"text\": \"\\ufb01ne-tuned on vision-and-language downstream tasks where\",\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pdf_text_example\\vertical_en_blocks.json, Line 5692: \"text\": \"tially embedded in a dense form alongside language tokens.\",\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pre_proc_results\\2列_ViLT_1_title.pdf\\preproc_out.json, Line 454: \"text\": \"language downstream tasks. Current approaches\",\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pre_proc_results\\2列_ViLT_1_title.pdf\\preproc_out.json, Line 1313: \"text\": \"joint domain of vision and language, giving birth to the cat-\",\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pre_proc_results\\2列_ViLT_1_title.pdf\\preproc_out.json, Line 2957: \"text\": \"with image text matching and masked language modeling\",\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pre_proc_results\\2列_ViLT_1_title.pdf\\preproc_out.json, Line 3082: \"text\": \"ﬁne-tuned on vision-and-language downstream tasks where\",\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pre_proc_results\\2列_ViLT_1_title.pdf\\preproc_out.json, Line 3196: \"text\": \"tially embedded in a dense form alongside language tokens.\",\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pre_proc_results\\2列_ViLT_1_title.pdf\\preproc_out.json, Line 4162: \"text\": \"language downstream tasks. Current approaches\",\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pre_proc_results\\2列_ViLT_1_title.pdf\\preproc_out.json, Line 5021: \"text\": \"joint domain of vision and language, giving birth to the cat-\",\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pre_proc_results\\2列_ViLT_1_title.pdf\\preproc_out.json, Line 9164: \"text\": \"with image text matching and masked language modeling\",\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pre_proc_results\\2列_ViLT_1_title.pdf\\preproc_out.json, Line 9289: \"text\": \"ﬁne-tuned on vision-and-language downstream tasks where\",\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pre_proc_results\\2列_ViLT_1_title.pdf\\preproc_out.json, Line 9403: \"text\": \"tially embedded in a dense form alongside language tokens.\",\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pre_proc_results\\纯2列_10.1002_mrm.24141_4.pdf\\preproc_out.json, Line 1394: \"text\": \"standard 1.5T clinical MRI scanner (Siemens, Erlangen,\",\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pre_proc_results\\纯2列_10.1002_mrm.24141_4.pdf\\preproc_out.json, Line 7126: \"text\": \"standard 1.5T clinical MRI scanner (Siemens, Erlangen,\",\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pre_proc_results\\纯2列_ViLT_6_文字_表格.pdf\\preproc_out.json, Line 199: \"text\": \"and-language downstream tasks: for\",\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pre_proc_results\\纯2列_ViLT_6_文字_表格.pdf\\preproc_out.json, Line 2501: \"text\": \"performance gain for vision and language downstream tasks.\",\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pre_proc_results\\纯2列_ViLT_6_文字_表格.pdf\\preproc_out.json, Line 3128: \"text\": \"language. The annotated answers are originally in free-form\",\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pre_proc_results\\纯2列_ViLT_6_文字_表格.pdf\\preproc_out.json, Line 3162: \"text\": \"natural language, but it is a common practice to convert the\",\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pre_proc_results\\纯2列_ViLT_6_文字_表格.pdf\\preproc_out.json, Line 5068: \"text\": \"and-language downstream tasks: for\",\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pre_proc_results\\纯2列_ViLT_6_文字_表格.pdf\\preproc_out.json, Line 7370: \"text\": \"performance gain for vision and language downstream tasks.\",\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pre_proc_results\\纯2列_ViLT_6_文字_表格.pdf\\preproc_out.json, Line 11423: \"text\": \"language. The annotated answers are originally in free-form\",\n",
    "File: D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/MinerU\\tests\\assets\\pre_proc_results\\纯2列_ViLT_6_文字_表格.pdf\\preproc_out.json, Line 11457: \"text\": \"natural language, but it is a common practice to convert the\",\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['model', 'trainer', 'iteration'])\n",
      "Key: model, Value Type: <class 'collections.OrderedDict'>\n",
      "Key: trainer, Value Type: <class 'dict'>\n",
      "Key: iteration, Value Type: <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Đường dẫn đến file .pth của bạn\n",
    "model_path = \"D:/OneDrive - Hanoi University of Science and Technology/ITE10-DS&AI-HUST/Learn&Task/AI_Machine_Deep/AI-ML-DL/20232 NLP - M8 AIO/2024_PDF_Parser_CODEMELY/PDF-Extract-Kit/models/Layout/model_final.pth\"\n",
    "\n",
    "# Tải nội dung từ file .pth\n",
    "model_data = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "\n",
    "# In ra các khóa (keys) có trong file để xem mô hình chứa những gì\n",
    "print(model_data.keys())\n",
    "\n",
    "# Kiểm tra nếu mô hình là một từ điển (dictionary), in ra chi tiết hơn\n",
    "if isinstance(model_data, dict):\n",
    "    for key, value in model_data.items():\n",
    "        print(f\"Key: {key}, Value Type: {type(value)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: backbone.fpn_lateral2.weight | Size: torch.Size([256, 768, 1, 1]) | Values : tensor([[[[ 0.0493]],\n",
      "\n",
      "         [[-0.0123]],\n",
      "\n",
      "         [[ 0.0371]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0031]],\n",
      "\n",
      "         [[-0.0039]],\n",
      "\n",
      "         [[-0.0297]]],\n",
      "\n",
      "\n",
      "        [[[-0.0462]],\n",
      "\n",
      "         [[ 0.0073]],\n",
      "\n",
      "         [[ 0.0399]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0447]],\n",
      "\n",
      "         [[ 0.0489]],\n",
      "\n",
      "         [[ 0.0065]]]]) \n",
      "\n",
      "Layer: backbone.fpn_lateral2.bias | Size: torch.Size([256]) | Values : tensor([-0.0131, -0.0220]) \n",
      "\n",
      "Layer: backbone.fpn_output2.weight | Size: torch.Size([256, 256, 3, 3]) | Values : tensor([[[[ 0.0081,  0.0146,  0.0122],\n",
      "          [-0.0102, -0.0078, -0.0085],\n",
      "          [ 0.0191, -0.0018, -0.0273]],\n",
      "\n",
      "         [[-0.0030,  0.0136, -0.0145],\n",
      "          [ 0.0267, -0.0160,  0.0169],\n",
      "          [-0.0018,  0.0263,  0.0124]],\n",
      "\n",
      "         [[-0.0121, -0.0026,  0.0006],\n",
      "          [-0.0189,  0.0254,  0.0300],\n",
      "          [-0.0226,  0.0062, -0.0145]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0441, -0.0220, -0.0462],\n",
      "          [ 0.0137, -0.0316, -0.0156],\n",
      "          [ 0.0095,  0.0437, -0.0104]],\n",
      "\n",
      "         [[-0.0278, -0.0262, -0.0099],\n",
      "          [ 0.0228, -0.0008, -0.0082],\n",
      "          [ 0.0487,  0.0198,  0.0199]],\n",
      "\n",
      "         [[ 0.0260,  0.0180,  0.0339],\n",
      "          [-0.0115,  0.0390,  0.0466],\n",
      "          [ 0.0341,  0.0082,  0.0253]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0389,  0.0162, -0.0007],\n",
      "          [-0.0060, -0.0072, -0.0539],\n",
      "          [ 0.0161,  0.0009, -0.0059]],\n",
      "\n",
      "         [[ 0.0152,  0.0157,  0.0086],\n",
      "          [ 0.0145, -0.0133, -0.0010],\n",
      "          [ 0.0333,  0.0231,  0.0007]],\n",
      "\n",
      "         [[-0.0078, -0.0220,  0.0110],\n",
      "          [ 0.0095, -0.0288,  0.0190],\n",
      "          [-0.0259, -0.0358,  0.0164]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0245,  0.0381, -0.0087],\n",
      "          [-0.0112,  0.0113, -0.0231],\n",
      "          [ 0.0068,  0.0307,  0.0326]],\n",
      "\n",
      "         [[-0.0151, -0.0216, -0.0156],\n",
      "          [-0.0245,  0.0287, -0.0156],\n",
      "          [-0.0023,  0.0210, -0.0237]],\n",
      "\n",
      "         [[ 0.0200,  0.0013, -0.0498],\n",
      "          [ 0.0259, -0.0217, -0.0308],\n",
      "          [ 0.0498,  0.0020, -0.0296]]]]) \n",
      "\n",
      "Layer: backbone.fpn_output2.bias | Size: torch.Size([256]) | Values : tensor([0.0194, 0.0241]) \n",
      "\n",
      "Layer: backbone.fpn_lateral3.weight | Size: torch.Size([256, 768, 1, 1]) | Values : tensor([[[[ 0.0407]],\n",
      "\n",
      "         [[-0.0342]],\n",
      "\n",
      "         [[-0.0428]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0092]],\n",
      "\n",
      "         [[-0.0020]],\n",
      "\n",
      "         [[-0.0437]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0199]],\n",
      "\n",
      "         [[ 0.0025]],\n",
      "\n",
      "         [[-0.0032]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0084]],\n",
      "\n",
      "         [[ 0.0310]],\n",
      "\n",
      "         [[ 0.0323]]]]) \n",
      "\n",
      "Layer: backbone.fpn_lateral3.bias | Size: torch.Size([256]) | Values : tensor([-0.0139, -0.0150]) \n",
      "\n",
      "Layer: backbone.fpn_output3.weight | Size: torch.Size([256, 256, 3, 3]) | Values : tensor([[[[ 3.7674e-02, -2.1729e-02, -1.4118e-02],\n",
      "          [ 1.8511e-02,  4.0092e-03, -1.8472e-02],\n",
      "          [ 1.4929e-02,  8.2761e-03, -2.2837e-02]],\n",
      "\n",
      "         [[-7.2130e-04,  4.7565e-03, -2.5605e-02],\n",
      "          [ 1.2064e-02,  1.2748e-02,  1.5465e-02],\n",
      "          [ 2.1006e-03, -2.3971e-02, -1.7686e-02]],\n",
      "\n",
      "         [[ 5.5273e-03,  1.1612e-02, -3.3312e-02],\n",
      "          [-9.8458e-03,  1.1813e-02,  3.0658e-03],\n",
      "          [ 1.0656e-02, -3.4729e-02, -3.8544e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.3877e-03,  9.2319e-03, -7.7837e-03],\n",
      "          [ 1.9954e-02,  4.0006e-04, -1.0323e-02],\n",
      "          [ 1.3170e-02,  3.7737e-03,  5.1801e-03]],\n",
      "\n",
      "         [[-4.2339e-02, -5.5368e-03,  1.2236e-03],\n",
      "          [ 7.5381e-05,  3.0855e-02,  1.1375e-03],\n",
      "          [-1.7238e-02,  3.4370e-02,  2.8030e-02]],\n",
      "\n",
      "         [[ 2.3955e-02, -3.0150e-02, -1.5084e-02],\n",
      "          [ 1.1476e-02,  2.1661e-02,  2.7022e-02],\n",
      "          [ 3.3978e-02,  2.4108e-03,  5.3598e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 2.8653e-02,  7.6783e-03,  2.5089e-02],\n",
      "          [-1.8180e-02,  1.6284e-02,  3.6665e-02],\n",
      "          [-1.8980e-02,  1.8243e-02,  2.3931e-02]],\n",
      "\n",
      "         [[ 2.2000e-02,  6.1203e-03, -8.2588e-03],\n",
      "          [ 2.4354e-02, -8.4657e-03, -3.5407e-02],\n",
      "          [ 2.7495e-02,  1.5705e-02,  3.3889e-03]],\n",
      "\n",
      "         [[-1.5739e-02, -2.2967e-02, -1.8522e-03],\n",
      "          [-8.1233e-03, -2.7171e-02, -1.5704e-02],\n",
      "          [ 8.1704e-03, -1.3154e-02,  9.5145e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-8.8014e-03,  1.3248e-02, -6.8321e-03],\n",
      "          [ 4.6864e-02,  2.3013e-02,  3.4291e-02],\n",
      "          [ 3.9019e-02, -1.5670e-03, -1.3440e-02]],\n",
      "\n",
      "         [[ 2.4555e-02,  9.3326e-03, -2.5565e-02],\n",
      "          [-1.4837e-02, -1.6606e-02, -4.6870e-02],\n",
      "          [-2.2300e-03,  3.9262e-03,  2.6914e-02]],\n",
      "\n",
      "         [[ 4.9089e-02, -1.1665e-03,  1.2295e-02],\n",
      "          [ 3.0448e-02,  2.1275e-02,  1.0005e-02],\n",
      "          [ 1.2016e-02, -1.5805e-03,  2.0226e-02]]]]) \n",
      "\n",
      "Layer: backbone.fpn_output3.bias | Size: torch.Size([256]) | Values : tensor([0.0125, 0.0404]) \n",
      "\n",
      "Layer: backbone.fpn_lateral4.weight | Size: torch.Size([256, 768, 1, 1]) | Values : tensor([[[[-0.0660]],\n",
      "\n",
      "         [[-0.0473]],\n",
      "\n",
      "         [[ 0.0082]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0236]],\n",
      "\n",
      "         [[ 0.0031]],\n",
      "\n",
      "         [[ 0.0025]]],\n",
      "\n",
      "\n",
      "        [[[-0.0234]],\n",
      "\n",
      "         [[ 0.0234]],\n",
      "\n",
      "         [[ 0.0089]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0578]],\n",
      "\n",
      "         [[ 0.0282]],\n",
      "\n",
      "         [[ 0.0582]]]]) \n",
      "\n",
      "Layer: backbone.fpn_lateral4.bias | Size: torch.Size([256]) | Values : tensor([ 0.0054, -0.0014]) \n",
      "\n",
      "Layer: backbone.fpn_output4.weight | Size: torch.Size([256, 256, 3, 3]) | Values : tensor([[[[ 0.0081,  0.0134, -0.0242],\n",
      "          [ 0.0283, -0.0282, -0.0412],\n",
      "          [-0.0009,  0.0060, -0.0246]],\n",
      "\n",
      "         [[ 0.0421,  0.0014, -0.0187],\n",
      "          [ 0.0294, -0.0029,  0.0163],\n",
      "          [ 0.0290,  0.0058, -0.0162]],\n",
      "\n",
      "         [[ 0.0257,  0.0097,  0.0123],\n",
      "          [-0.0030, -0.0047,  0.0117],\n",
      "          [-0.0177, -0.0188, -0.0269]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0012, -0.0283, -0.0096],\n",
      "          [-0.0022, -0.0381, -0.0473],\n",
      "          [ 0.0256, -0.0120,  0.0163]],\n",
      "\n",
      "         [[-0.0463, -0.0239, -0.0047],\n",
      "          [-0.0296,  0.0008,  0.0161],\n",
      "          [-0.0379, -0.0004,  0.0290]],\n",
      "\n",
      "         [[ 0.0379, -0.0187,  0.0096],\n",
      "          [ 0.0257, -0.0063, -0.0504],\n",
      "          [ 0.0184, -0.0209, -0.0182]]],\n",
      "\n",
      "\n",
      "        [[[-0.0149,  0.0052, -0.0161],\n",
      "          [-0.0199, -0.0378,  0.0196],\n",
      "          [ 0.0022,  0.0180,  0.0211]],\n",
      "\n",
      "         [[ 0.0208, -0.0274,  0.0023],\n",
      "          [ 0.0219, -0.0143,  0.0087],\n",
      "          [-0.0099, -0.0286, -0.0099]],\n",
      "\n",
      "         [[-0.0188, -0.0421, -0.0183],\n",
      "          [-0.0284, -0.0211,  0.0127],\n",
      "          [ 0.0239, -0.0194, -0.0250]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0026,  0.0263,  0.0181],\n",
      "          [ 0.0097,  0.0412,  0.0089],\n",
      "          [ 0.0095,  0.0454,  0.0197]],\n",
      "\n",
      "         [[-0.0102, -0.0138,  0.0312],\n",
      "          [-0.0299, -0.0093, -0.0341],\n",
      "          [-0.0244,  0.0050, -0.0083]],\n",
      "\n",
      "         [[ 0.0255,  0.0043,  0.0264],\n",
      "          [-0.0056,  0.0124, -0.0367],\n",
      "          [-0.0115, -0.0272, -0.0346]]]]) \n",
      "\n",
      "Layer: backbone.fpn_output4.bias | Size: torch.Size([256]) | Values : tensor([0.0182, 0.0459]) \n",
      "\n",
      "Layer: backbone.fpn_lateral5.weight | Size: torch.Size([256, 768, 1, 1]) | Values : tensor([[[[ 0.0230]],\n",
      "\n",
      "         [[ 0.0555]],\n",
      "\n",
      "         [[ 0.0453]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0423]],\n",
      "\n",
      "         [[ 0.0105]],\n",
      "\n",
      "         [[ 0.0374]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0200]],\n",
      "\n",
      "         [[ 0.0050]],\n",
      "\n",
      "         [[ 0.0393]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0343]],\n",
      "\n",
      "         [[-0.0140]],\n",
      "\n",
      "         [[ 0.0581]]]]) \n",
      "\n",
      "Layer: backbone.fpn_lateral5.bias | Size: torch.Size([256]) | Values : tensor([0.0100, 0.0019]) \n",
      "\n",
      "Layer: backbone.fpn_output5.weight | Size: torch.Size([256, 256, 3, 3]) | Values : tensor([[[[ 1.4049e-02, -5.3826e-02,  7.4054e-03],\n",
      "          [ 2.8529e-02, -5.2541e-02, -4.3536e-02],\n",
      "          [-1.7527e-02, -2.9891e-02, -2.1554e-02]],\n",
      "\n",
      "         [[-9.9154e-03, -1.8961e-02, -3.1996e-02],\n",
      "          [ 2.2812e-02, -5.1380e-03,  8.0618e-03],\n",
      "          [ 4.7076e-03, -2.5482e-02,  1.0167e-02]],\n",
      "\n",
      "         [[ 2.0339e-02,  6.0269e-03, -4.2245e-03],\n",
      "          [ 1.0928e-02,  1.4218e-02,  4.5236e-02],\n",
      "          [-5.3996e-04,  1.9761e-02,  1.4016e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.6269e-02, -4.3323e-03,  5.1197e-03],\n",
      "          [ 1.0437e-02, -3.8802e-02, -1.1508e-02],\n",
      "          [-3.4363e-03, -8.7169e-03,  1.0367e-02]],\n",
      "\n",
      "         [[-4.8631e-02,  3.8170e-02,  6.9270e-02],\n",
      "          [-2.5905e-02,  2.2064e-03,  7.8408e-02],\n",
      "          [-6.0978e-02, -1.1988e-02,  3.7685e-02]],\n",
      "\n",
      "         [[ 5.1072e-02,  4.6922e-02,  2.1191e-02],\n",
      "          [ 1.7284e-03,  1.1496e-02,  2.7379e-03],\n",
      "          [ 2.6291e-02, -2.2064e-02, -1.6318e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 3.8333e-03, -4.9873e-02, -2.7900e-02],\n",
      "          [ 6.7892e-05, -2.3640e-02,  1.3410e-02],\n",
      "          [ 2.1519e-02, -4.4794e-02,  1.9508e-02]],\n",
      "\n",
      "         [[-2.6634e-03, -2.5998e-02,  1.1252e-03],\n",
      "          [ 9.3991e-03, -2.5608e-02, -2.2381e-03],\n",
      "          [-6.5165e-03, -3.5306e-03, -2.6876e-02]],\n",
      "\n",
      "         [[ 3.6535e-02,  3.1230e-02, -6.9529e-03],\n",
      "          [ 1.6364e-02,  3.4425e-02,  1.3728e-02],\n",
      "          [ 1.7762e-03, -7.0584e-04,  2.4166e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.4774e-02,  1.7244e-02,  5.2899e-02],\n",
      "          [-1.6969e-02,  5.9685e-03,  3.0749e-02],\n",
      "          [ 1.2232e-02,  3.1616e-02,  5.9853e-02]],\n",
      "\n",
      "         [[-3.0313e-02, -3.5148e-02, -6.3524e-02],\n",
      "          [-4.8644e-02, -4.7796e-02, -5.8314e-02],\n",
      "          [-8.4979e-03, -1.0229e-02, -2.8572e-02]],\n",
      "\n",
      "         [[ 1.2706e-02,  4.1255e-02,  5.1324e-02],\n",
      "          [-1.9789e-03,  5.8702e-03, -1.1831e-02],\n",
      "          [-4.3240e-03, -4.8838e-03,  1.2368e-02]]]]) \n",
      "\n",
      "Layer: backbone.fpn_output5.bias | Size: torch.Size([256]) | Values : tensor([0.0185, 0.0564]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.cls_token | Size: torch.Size([1, 1, 768]) | Values : tensor([[[ 7.5734e-03, -2.7692e-03, -1.5134e-02,  1.7997e-02,  9.5880e-03,\n",
      "          -2.5005e-03, -4.0110e-02, -6.3430e-02,  9.4368e-03,  3.5689e-03,\n",
      "           4.5270e-03,  9.5906e-04, -3.2695e-02, -9.2099e-03,  2.9344e-03,\n",
      "           4.6607e-03,  4.3975e-03, -2.8766e-03,  1.6091e-02, -9.4209e-03,\n",
      "           1.4871e-03,  1.0696e-02,  1.3306e-02, -1.2157e-02,  1.0858e-02,\n",
      "           8.4808e-04, -1.5516e-03, -5.1021e-03, -8.3159e-03,  1.3884e-02,\n",
      "          -5.8113e-03,  2.1554e-03, -3.3025e-03,  4.2445e-03,  5.3483e-03,\n",
      "           4.2702e-03, -2.1104e-02,  1.1185e-03,  8.2382e-03, -5.5630e-03,\n",
      "          -1.7738e-03, -3.9180e-03,  9.2023e-03,  1.1955e-02,  3.7624e-04,\n",
      "           6.6147e-03, -6.7004e-03,  4.6111e-03, -7.1119e-03,  1.0322e-02,\n",
      "           4.9707e-03,  8.6684e-03, -3.6343e-03, -1.1068e-02,  7.2649e-03,\n",
      "          -6.3751e-03,  4.9274e-03, -3.7336e-03,  2.1374e-04,  1.3526e-04,\n",
      "           3.8156e-03, -4.1416e-03, -2.8967e-02,  5.0977e-03,  5.6670e-03,\n",
      "          -6.4374e-03,  1.9840e-02,  1.4087e-02, -7.0563e-03,  9.8888e-03,\n",
      "           1.2128e-02, -1.1836e-02, -4.6098e-03,  5.4120e-03,  9.7583e-03,\n",
      "           9.6513e-03,  4.8915e-03, -2.4121e-02,  6.6543e-03,  7.6360e-03,\n",
      "           3.5110e-02,  6.6437e-03, -3.2380e-03,  6.1574e-03,  3.3144e-03,\n",
      "           3.3771e-04,  5.8499e-03,  2.3159e-02,  8.6188e-04,  3.4953e-03,\n",
      "           8.7392e-03,  9.1838e-03,  3.7778e-03, -1.4507e-02, -6.6771e-03,\n",
      "          -1.0397e-03,  2.7141e-03,  3.0286e-03,  2.5663e-02,  1.4670e-03,\n",
      "          -2.0232e-03,  1.7636e-02,  7.0791e-03, -7.7977e-03, -1.2107e-02,\n",
      "          -1.1003e-02,  2.1153e-02, -6.9120e-03, -1.6372e-03,  6.3119e-03,\n",
      "          -1.0607e-02,  5.1039e-03, -2.3610e-03,  4.7823e-03,  5.3949e-03,\n",
      "          -2.5745e-03,  1.4250e-04, -3.2973e-03,  2.8695e-03, -8.9190e-04,\n",
      "          -2.6976e-03,  6.3950e-03,  1.6755e-03, -9.9826e-03, -4.6622e-03,\n",
      "          -2.8588e-03, -1.1639e-02,  1.9967e-03,  1.8764e-04,  5.3446e-03,\n",
      "           1.6023e-03,  1.7366e-03, -1.1198e-02,  3.2825e-03,  3.8598e-03,\n",
      "          -5.2343e-04, -9.7708e-03, -2.5118e-02,  1.3370e-02,  6.1245e-03,\n",
      "           6.1265e-03,  2.1546e-03, -3.6366e-02,  7.5051e-03, -9.5432e-03,\n",
      "          -3.1639e-02,  6.5933e-04,  3.8244e-03, -3.1787e-03,  2.7653e-03,\n",
      "           1.4102e-02, -9.5429e-03,  4.4548e-02, -3.9236e-04, -1.6422e-02,\n",
      "           9.4810e-03,  3.6673e-03,  1.1527e-02,  1.4800e-03, -9.9934e-04,\n",
      "          -6.0215e-03, -7.2149e-03,  7.2721e-03, -1.7095e-03,  4.0227e-03,\n",
      "          -1.6038e-02, -4.7575e-03,  9.8674e-03,  2.3247e-03,  1.1868e-02,\n",
      "          -2.3337e-02, -9.7612e-04,  1.2583e-02,  1.7525e-03, -2.5476e-03,\n",
      "          -5.1979e-03, -6.1032e-03, -2.3165e-03,  9.2666e-03, -4.9022e-03,\n",
      "           1.0304e-02,  3.7749e-03, -5.4178e-03,  6.7780e-03,  6.6430e-02,\n",
      "           1.0566e-03, -1.6308e-02, -5.7073e-03, -5.4908e-03, -2.3886e-03,\n",
      "           5.3181e-03,  6.8387e-03,  6.4158e-04,  4.3713e-02,  9.2120e-03,\n",
      "           4.9934e-03, -6.4539e-03, -5.7651e-03,  1.9185e-02,  8.2388e-03,\n",
      "          -1.5445e-02, -1.8329e-03, -1.7300e-02,  2.3595e-03,  7.2076e-03,\n",
      "          -6.0184e-03, -1.3855e-02, -6.3397e-03,  8.3222e-03,  3.4991e-03,\n",
      "          -1.3343e-02, -1.4302e-02, -5.7467e-03, -1.1872e-02, -1.0893e-02,\n",
      "          -7.1294e-03,  1.5310e-02, -9.1776e-03,  3.7938e-03,  1.0114e-03,\n",
      "           1.4793e-02, -5.8188e-03, -3.0194e-04, -8.6444e-03,  1.3029e-02,\n",
      "           2.3622e-02, -4.1167e-03,  5.3588e-03, -8.7518e-04, -8.3008e-03,\n",
      "          -9.1341e-03,  3.3873e-03, -8.7225e-03,  4.9299e-03,  8.4040e-03,\n",
      "           8.1337e-03, -3.1706e-03, -4.1790e-02,  6.4031e-03,  3.9634e-03,\n",
      "           6.5608e-03,  9.9925e-03,  2.6247e-03, -2.6712e-03, -7.8139e-03,\n",
      "           1.0441e-02,  1.3509e-02, -1.7086e-02, -4.1310e-03, -1.3377e-02,\n",
      "          -9.0682e-04, -7.6064e-03,  2.9792e-03,  6.2314e-03, -2.7181e-03,\n",
      "           7.9332e-03, -1.6924e-02,  2.9520e-03,  1.5900e-02, -2.6642e-01,\n",
      "          -9.4261e-04,  2.9739e-03,  1.9161e-02,  9.0485e-03,  1.7127e-02,\n",
      "           5.0680e-03, -1.3819e-03, -3.1484e-03,  6.5963e-03, -7.2133e-03,\n",
      "           1.7409e-03,  8.2716e-03,  1.2996e-03,  3.9432e-04,  2.0334e-02,\n",
      "          -9.0748e-03,  7.4742e-03, -4.4917e-03, -2.7544e-03, -6.4221e-04,\n",
      "          -2.2687e-03, -9.2021e-03, -1.6344e-03,  1.2233e-02,  5.7288e-03,\n",
      "           1.6112e-02, -6.4473e-03,  8.0529e-03,  9.3031e-03, -8.9609e-04,\n",
      "           8.3007e-03, -1.7703e-02, -8.7246e-03, -1.2376e-02,  2.5237e-03,\n",
      "           4.1404e-03, -6.7346e-03, -1.2341e-03, -3.3151e-04,  4.9351e-03,\n",
      "           9.6756e-03,  8.2133e-03,  3.5762e-03,  6.4370e-03,  2.6154e-03,\n",
      "          -3.6572e-03, -3.3994e-02,  1.0804e-02,  1.8755e-03,  5.3293e-03,\n",
      "          -1.0071e-02, -1.5093e-02, -6.2304e-03, -6.1262e-04,  6.4123e-03,\n",
      "           2.2271e-03,  4.7572e-03, -5.2708e-02, -1.8739e-03,  1.0878e-02,\n",
      "           5.1843e-03, -4.8294e-03,  3.0857e-03, -4.1535e-03,  1.6519e-02,\n",
      "           2.4042e-03, -2.0776e-03,  6.5953e-03,  2.4402e-04,  2.7222e-02,\n",
      "          -2.8073e-03, -8.9409e-03,  2.1252e-02, -5.3797e-04,  6.3596e-03,\n",
      "           2.5572e-02,  4.6229e-03, -1.3484e-02,  1.7279e-02,  9.7261e-03,\n",
      "           5.3964e-03, -1.6246e-03,  1.1699e-03,  2.6265e-03,  1.5706e-03,\n",
      "           1.4203e-03, -9.3026e-03, -9.9490e-03, -4.0845e-03,  6.1998e-03,\n",
      "          -1.2059e-02, -1.1559e-02,  6.4787e-03,  1.7112e-02, -2.3313e-03,\n",
      "           7.4008e-06, -2.6135e-03,  1.0359e-02, -1.3972e-03,  1.2698e-03,\n",
      "           4.8261e-03,  4.5690e-03,  7.5706e-03, -2.7736e-03, -2.2065e-03,\n",
      "          -8.9406e-03, -5.0920e-03,  3.7991e-03,  4.8679e-03,  1.1704e-02,\n",
      "           2.6937e-02, -4.0402e-03, -1.4803e-03,  1.1520e-03, -7.4065e-03,\n",
      "          -9.3096e-04,  4.7348e-03, -9.9661e-04, -1.3340e-02, -8.9140e-03,\n",
      "          -2.0788e-02,  1.1483e-03, -1.6943e-04,  3.4471e-03,  1.7748e-02,\n",
      "           3.9882e-04,  2.2709e-03,  1.4389e-03,  1.2547e-03,  9.1620e-03,\n",
      "          -3.8711e-03, -7.6610e-04, -6.6751e-04,  2.6827e-04,  1.2743e-02,\n",
      "          -1.0552e-02,  4.5271e-03, -3.8963e-03,  1.2831e-02, -5.0608e-03,\n",
      "           8.5010e-04,  2.4364e-02,  1.2625e-02, -5.7222e-03,  3.6802e-03,\n",
      "          -1.2902e-02,  2.3678e-03,  9.9713e-03,  9.3843e-04,  5.0723e-03,\n",
      "           1.0487e-02,  4.6803e-03,  6.4368e-03, -2.5979e-02, -1.2663e-02,\n",
      "           6.2655e-03, -9.7439e-03,  2.6395e-02, -1.0285e-02, -4.1587e-03,\n",
      "           1.3845e-03,  4.1672e-03,  5.2951e-03,  4.4425e-03,  2.1080e-02,\n",
      "           1.3819e-02,  6.1602e-03,  1.1384e-02,  4.9992e-03, -7.3131e-03,\n",
      "           2.0391e-03,  1.0242e-02, -1.1078e-02,  7.3632e-03,  5.8221e-03,\n",
      "          -4.7986e-03,  2.3716e-03, -8.6446e-03, -4.0864e-03, -1.4984e-02,\n",
      "          -1.1118e-02,  1.7026e-02, -1.3757e-02, -1.8982e-03,  6.4130e-03,\n",
      "           9.0289e-03, -1.1078e-02, -2.9979e-02,  4.3558e-03, -1.8525e-02,\n",
      "          -1.9815e-02, -7.0925e-04, -1.0636e-02, -2.3946e-02, -4.4640e-03,\n",
      "           6.5053e-03,  3.0933e-03,  6.6742e-04,  3.9510e-03,  6.2792e-02,\n",
      "           5.2323e-03,  5.7896e-03, -4.2090e-03, -1.3331e-03, -6.8403e-04,\n",
      "          -1.9287e-03,  4.9773e-03, -1.1680e-02, -1.8550e-03,  7.4365e-03,\n",
      "          -1.1731e-02,  9.5370e-03, -1.4914e-03,  8.1264e-03, -3.8802e-03,\n",
      "          -1.9693e-02,  6.2530e-03,  9.1221e-03,  1.0140e-02,  8.4622e-03,\n",
      "          -1.5892e-04,  1.7737e-02, -2.7450e-03,  1.2227e-03, -7.3507e-03,\n",
      "          -1.3319e-02,  7.6553e-05, -8.7236e-03, -2.0530e-03,  1.1980e-02,\n",
      "           6.8910e-03,  9.8800e-03,  1.4559e-04, -7.1976e-03,  5.1763e-03,\n",
      "          -1.0255e-02,  7.1996e-03,  1.3297e-02, -1.0706e-02, -1.1235e-03,\n",
      "          -8.4357e-03, -1.7019e-03,  2.8727e-03,  1.7099e-02,  4.1719e-03,\n",
      "           1.1410e-02, -3.8162e-03,  2.3974e-02,  1.5158e-02,  1.2597e-02,\n",
      "           6.8925e-03,  4.8944e-04,  1.8078e-02, -1.3960e-02, -4.2125e-03,\n",
      "           6.2953e-03,  1.0869e-04,  9.6037e-03, -1.8313e-03,  1.1227e-02,\n",
      "           3.3652e-03,  4.8352e-03, -1.1735e-03, -7.8813e-03,  1.3705e-03,\n",
      "          -4.1361e-03,  6.6770e-03, -4.6258e-05, -9.0118e-03,  1.2124e-02,\n",
      "          -1.9819e-02, -1.3524e-02, -1.6016e-02, -3.5921e-03,  1.0001e-02,\n",
      "           8.6919e-03,  1.9264e-03, -2.9401e-03,  1.4794e-03, -5.9240e-03,\n",
      "          -8.3625e-03, -9.3732e-04,  2.8097e-05,  1.2768e-04,  2.1056e-02,\n",
      "          -1.9384e-02,  4.0767e-03,  6.4765e-03, -6.2157e-03, -3.5277e-03,\n",
      "          -1.1482e-02,  2.8064e-04,  1.8571e-02,  5.5901e-03,  4.0928e-03,\n",
      "           5.2761e-03,  7.7745e-03,  3.9046e-03, -3.6599e-03, -1.7574e-02,\n",
      "          -7.3305e-03,  1.5551e-02,  1.4551e-02,  5.1959e-03,  3.1807e-03,\n",
      "          -1.2840e-02, -1.2437e-02, -5.5805e-03,  2.2823e-03,  4.9938e-03,\n",
      "          -3.2044e-02,  2.7934e-02,  2.5004e-02, -7.7389e-03,  1.2652e-02,\n",
      "           1.6323e-03,  2.0111e-02,  1.5157e-02, -9.6708e-03,  6.2930e-03,\n",
      "           1.1255e-02,  3.3178e-04, -2.7987e-03,  8.5102e-03, -1.7835e-02,\n",
      "           2.8710e-02, -7.3313e-03, -1.8121e-03, -4.8712e-01,  3.9793e-03,\n",
      "          -1.1404e-02,  6.1754e-03,  5.1468e-03, -6.2420e-03,  8.5094e-03,\n",
      "           1.9192e-02,  5.5491e-03, -1.5066e-02,  3.6750e-02, -1.5842e-02,\n",
      "           2.1582e-03,  1.4104e-02,  3.0065e-02,  4.5212e-03, -2.3904e-04,\n",
      "           1.3461e-02,  1.2566e-02, -2.2414e-03,  4.4712e-03,  9.1584e-03,\n",
      "          -8.0062e-03, -3.0287e-03, -1.1207e-02,  5.2248e-03,  8.4324e-03,\n",
      "          -1.0179e-02,  2.6717e-02,  1.4754e-02,  2.0115e-03, -5.9847e-04,\n",
      "           1.1220e-03, -2.6254e-03,  3.8631e-02,  5.4672e-03,  8.1788e-04,\n",
      "           1.5058e-02,  1.8346e-02,  8.6906e-03, -1.4566e-02,  1.4633e-02,\n",
      "          -4.0170e-03, -1.9665e-03,  1.0030e-02, -1.0305e-02, -2.1915e-03,\n",
      "           6.8271e-03,  2.5170e-02,  1.9254e-02,  2.8919e-03,  4.0485e-03,\n",
      "          -2.8971e-03,  1.6928e-02, -2.3361e-02,  1.0837e-02, -2.2358e-03,\n",
      "          -1.8776e-02,  4.1210e-03,  1.8737e-02, -3.8465e-03,  8.8677e-04,\n",
      "          -8.2006e-03,  1.4288e-02,  2.7707e-02, -9.6076e-03,  2.0649e-02,\n",
      "           2.1768e-02,  1.1603e-02, -6.4381e-07,  4.6479e-02,  4.2279e-05,\n",
      "           1.9907e-03, -4.2980e-03,  3.1703e-03,  6.2422e-03,  2.7478e-03,\n",
      "           4.1792e-03,  5.9496e-03, -1.3348e-02,  7.6041e-03,  1.0071e-03,\n",
      "           5.6775e-03,  3.5386e-03, -1.5144e-02,  8.4525e-03,  7.9536e-03,\n",
      "           4.4147e-03, -7.7124e-03,  2.4706e-02,  1.1645e-02, -1.8015e-03,\n",
      "          -1.2072e-02, -5.8618e-03, -1.9532e-03, -1.1957e-02,  5.4356e-03,\n",
      "           1.6394e-02, -4.9727e-03,  3.4910e-03, -4.3265e-03,  4.5223e-03,\n",
      "           9.8589e-03, -5.8352e-03, -6.5735e-03, -5.1487e-03,  7.5172e-03,\n",
      "          -1.0472e-02,  5.3487e-02,  3.6403e-03,  1.0451e-02, -2.1276e-03,\n",
      "          -1.6567e-03,  1.2299e-02, -3.4049e-03, -1.1144e-02,  3.8780e-03,\n",
      "           9.0847e-03, -5.3438e-03,  1.4015e-02,  2.7255e-03,  2.0647e-03,\n",
      "           1.1006e-03, -1.6911e-02,  5.0275e-03,  5.1547e-03,  1.6466e-02,\n",
      "           3.2656e-03,  2.8896e-02,  1.5279e-02, -4.1225e-03,  1.1081e-02,\n",
      "           3.4697e-02, -6.9359e-03, -1.7151e-03,  2.4760e-02, -1.2075e-02,\n",
      "          -7.3373e-03,  1.3574e-02,  6.1506e-03,  7.0509e-03,  3.5707e-03,\n",
      "          -4.6423e-03,  5.4365e-03, -1.4284e-03,  1.0465e-02,  5.6180e-04,\n",
      "          -1.6682e-02,  9.1731e-03, -2.3664e-04, -4.0248e-03,  2.2532e-02,\n",
      "          -1.7195e-02,  2.8602e-02, -3.4021e-03,  1.5920e-02, -7.6234e-03,\n",
      "          -1.4111e-02,  2.4936e-03, -2.1979e-02,  4.4878e-03,  6.5401e-03,\n",
      "           1.4544e-02,  5.7070e-03,  1.0195e-02,  7.1242e-03,  5.5980e-03,\n",
      "          -6.5659e-03,  2.1201e-02, -1.2291e-02, -2.6351e-02,  1.4785e-02,\n",
      "           1.2656e-03,  3.5336e-02,  2.5523e-03, -1.5238e-02, -4.7756e-03,\n",
      "           5.0560e-03, -1.5530e-02, -2.5897e-03]]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.pos_embed | Size: torch.Size([1, 197, 768]) | Values : tensor([[[ 0.0076, -0.0028, -0.0151,  ...,  0.0051, -0.0155, -0.0026],\n",
      "         [ 0.0056, -0.1007,  0.1487,  ..., -0.0356,  0.2486, -0.1445],\n",
      "         [-0.0266,  0.2978, -0.0881,  ..., -0.0894,  0.1394, -0.1730],\n",
      "         ...,\n",
      "         [-0.0372,  0.2722, -0.0518,  ..., -0.1120,  0.0856,  0.0423],\n",
      "         [ 0.0211,  0.0990,  0.0634,  ...,  0.0392,  0.0411,  0.1218],\n",
      "         [ 0.0467, -0.0434,  0.1670,  ..., -0.0534,  0.1204,  0.0705]]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.0.attention.self.query.weight | Size: torch.Size([768, 768]) | Values : tensor([[-0.1193, -0.0683,  0.0339,  ..., -0.0766, -0.0006,  0.0108],\n",
      "        [ 0.0176,  0.0342,  0.0581,  ...,  0.0972, -0.0900, -0.0873]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.0.attention.self.query.bias | Size: torch.Size([768]) | Values : tensor([ 0.2927, -0.5691]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.0.attention.self.key.weight | Size: torch.Size([768, 768]) | Values : tensor([[-0.0350, -0.1132, -0.0906,  ...,  0.0753,  0.1358,  0.0805],\n",
      "        [-0.0204,  0.0190,  0.1457,  ...,  0.0307,  0.0019, -0.0072]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.0.attention.self.key.bias | Size: torch.Size([768]) | Values : tensor([ 0.0077, -0.0086]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.0.attention.self.value.weight | Size: torch.Size([768, 768]) | Values : tensor([[-0.0195, -0.0183,  0.0048,  ...,  0.0009, -0.0055,  0.0022],\n",
      "        [-0.0223, -0.0122, -0.0406,  ..., -0.0050, -0.0296, -0.0025]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.0.attention.self.value.bias | Size: torch.Size([768]) | Values : tensor([0.0074, 0.0027]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.0.attention.output.dense.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0082,  0.0120, -0.0086,  ...,  0.0146,  0.0317, -0.0078],\n",
      "        [ 0.0002,  0.0223,  0.0209,  ..., -0.0176,  0.0123, -0.0227]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.0.attention.output.dense.bias | Size: torch.Size([768]) | Values : tensor([ 0.0098, -0.0045]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.0.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.3260, 0.4127]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.0.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([-0.0527, -0.0645]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.0.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Values : tensor([[ 0.0202, -0.0523,  0.0693,  ...,  0.0617, -0.0277, -0.0323],\n",
      "        [-0.0849, -0.0324, -0.0459,  ..., -0.0474,  0.0803, -0.0386]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.0.intermediate.dense.bias | Size: torch.Size([3072]) | Values : tensor([-0.0647, -0.1117]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.0.output.dense.weight | Size: torch.Size([768, 3072]) | Values : tensor([[-0.0100, -0.0405, -0.0367,  ..., -0.0404, -0.0021, -0.0024],\n",
      "        [-0.0521, -0.0273, -0.0067,  ..., -0.0156,  0.0144,  0.0218]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.0.output.dense.bias | Size: torch.Size([768]) | Values : tensor([-0.0019,  0.0287]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.0.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.3659, 0.2803]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.0.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([-0.0726, -0.0150]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.1.attention.self.query.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0023,  0.0620,  0.0427,  ..., -0.0339, -0.0323, -0.0497],\n",
      "        [ 0.0311,  0.0052,  0.0131,  ...,  0.0764, -0.0079, -0.0073]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.1.attention.self.query.bias | Size: torch.Size([768]) | Values : tensor([ 0.1700, -0.2715]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.1.attention.self.key.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0103,  0.0477, -0.1119,  ..., -0.0231, -0.0097, -0.0076],\n",
      "        [-0.0171,  0.1067,  0.0198,  ..., -0.0150,  0.0127,  0.0511]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.1.attention.self.key.bias | Size: torch.Size([768]) | Values : tensor([ 0.0235, -0.0160]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.1.attention.self.value.weight | Size: torch.Size([768, 768]) | Values : tensor([[-0.0128, -0.0121, -0.0052,  ...,  0.0090, -0.0141, -0.0084],\n",
      "        [ 0.0113,  0.0405, -0.0015,  ..., -0.0226,  0.0017, -0.0342]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.1.attention.self.value.bias | Size: torch.Size([768]) | Values : tensor([-0.0008, -0.0085]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.1.attention.output.dense.weight | Size: torch.Size([768, 768]) | Values : tensor([[-0.0273, -0.0105,  0.0144,  ..., -0.0163, -0.0045,  0.0115],\n",
      "        [ 0.0007,  0.0294, -0.0071,  ...,  0.0017,  0.0004,  0.0361]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.1.attention.output.dense.bias | Size: torch.Size([768]) | Values : tensor([-0.0098, -0.0107]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.1.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.4007, 0.3868]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.1.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([-0.1010, -0.0545]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.1.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Values : tensor([[-0.0539, -0.0136,  0.0519,  ..., -0.0231,  0.0319, -0.0190],\n",
      "        [ 0.0497, -0.0401,  0.0008,  ...,  0.0552,  0.0915,  0.0624]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.1.intermediate.dense.bias | Size: torch.Size([3072]) | Values : tensor([-0.1490, -0.1162]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.1.output.dense.weight | Size: torch.Size([768, 3072]) | Values : tensor([[-0.0753, -0.0449,  0.0140,  ..., -0.0611, -0.0181, -0.0262],\n",
      "        [ 0.0248,  0.0381, -0.0213,  ...,  0.0984, -0.0078, -0.0020]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.1.output.dense.bias | Size: torch.Size([768]) | Values : tensor([-0.0266, -0.0125]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.1.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.4488, 0.3442]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.1.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([0.0122, 0.0096]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.2.attention.self.query.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0306,  0.0840, -0.0553,  ..., -0.0740,  0.0011, -0.0283],\n",
      "        [ 0.0266,  0.0041,  0.0057,  ...,  0.0397, -0.0493, -0.0583]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.2.attention.self.query.bias | Size: torch.Size([768]) | Values : tensor([-0.2995, -0.2238]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.2.attention.self.key.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0155, -0.0610, -0.0992,  ..., -0.0964, -0.0078, -0.0643],\n",
      "        [-0.0502, -0.0283,  0.0301,  ...,  0.0587, -0.0623, -0.1015]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.2.attention.self.key.bias | Size: torch.Size([768]) | Values : tensor([-0.0521, -0.1098]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.2.attention.self.value.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0129,  0.0248,  0.0391,  ...,  0.0179,  0.0553, -0.0379],\n",
      "        [ 0.0565, -0.0129, -0.0113,  ...,  0.0379, -0.0027, -0.0102]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.2.attention.self.value.bias | Size: torch.Size([768]) | Values : tensor([ 0.0037, -0.0077]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.2.attention.output.dense.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0002, -0.0102, -0.0127,  ...,  0.0301, -0.0082, -0.0288],\n",
      "        [-0.0008,  0.0549, -0.0062,  ...,  0.0153,  0.0188, -0.0071]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.2.attention.output.dense.bias | Size: torch.Size([768]) | Values : tensor([-0.0262, -0.1300]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.2.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.5139, 0.3715]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.2.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([-0.0968, -0.0029]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.2.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Values : tensor([[-0.0186,  0.0326,  0.0589,  ...,  0.0009,  0.0030,  0.0084],\n",
      "        [-0.0009,  0.0273, -0.0315,  ...,  0.0021, -0.0052,  0.0188]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.2.intermediate.dense.bias | Size: torch.Size([3072]) | Values : tensor([-0.0877, -0.0348]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.2.output.dense.weight | Size: torch.Size([768, 3072]) | Values : tensor([[-0.0058, -0.0529, -0.0460,  ...,  0.0589, -0.0064, -0.0366],\n",
      "        [ 0.0506,  0.0008,  0.0270,  ..., -0.0199, -0.0018, -0.1543]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.2.output.dense.bias | Size: torch.Size([768]) | Values : tensor([0.0471, 0.1126]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.2.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.4608, 0.3178]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.2.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([0.0064, 0.0004]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.3.attention.self.query.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0211, -0.0506, -0.0812,  ...,  0.0126, -0.0315,  0.0537],\n",
      "        [ 0.0046, -0.0130, -0.0154,  ..., -0.0343, -0.0709, -0.0570]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.3.attention.self.query.bias | Size: torch.Size([768]) | Values : tensor([ 0.0207, -0.0921]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.3.attention.self.key.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0549, -0.1310, -0.0001,  ..., -0.0322, -0.0352,  0.0351],\n",
      "        [ 0.0458,  0.0674, -0.0743,  ...,  0.0035,  0.0103, -0.0045]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.3.attention.self.key.bias | Size: torch.Size([768]) | Values : tensor([-0.1257, -0.0760]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.3.attention.self.value.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0187,  0.0183, -0.0220,  ...,  0.0551,  0.0255, -0.0358],\n",
      "        [ 0.0431,  0.0119,  0.0321,  ..., -0.0070, -0.0223,  0.0094]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.3.attention.self.value.bias | Size: torch.Size([768]) | Values : tensor([-0.0081,  0.0166]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.3.attention.output.dense.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0066,  0.0248, -0.0540,  ..., -0.0244,  0.0082,  0.0178],\n",
      "        [-0.0412, -0.0269, -0.0086,  ...,  0.0543, -0.0021, -0.0050]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.3.attention.output.dense.bias | Size: torch.Size([768]) | Values : tensor([-0.0468, -0.0694]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.3.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.5774, 0.3087]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.3.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([-0.1087, -0.0459]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.3.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Values : tensor([[-0.0471,  0.0807, -0.0246,  ...,  0.0139,  0.0627,  0.0298],\n",
      "        [-0.0028,  0.0635,  0.0360,  ...,  0.0208,  0.1271, -0.0394]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.3.intermediate.dense.bias | Size: torch.Size([3072]) | Values : tensor([-0.0166, -0.1237]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.3.output.dense.weight | Size: torch.Size([768, 3072]) | Values : tensor([[-0.0377,  0.0119,  0.0080,  ..., -0.0467,  0.0366, -0.0254],\n",
      "        [ 0.0375, -0.0170, -0.0194,  ...,  0.0992,  0.0172,  0.0126]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.3.output.dense.bias | Size: torch.Size([768]) | Values : tensor([-0.0140,  0.1176]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.3.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.4972, 0.2208]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.3.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([-0.0095, -0.0034]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.4.attention.self.query.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0052,  0.0093,  0.0316,  ...,  0.0190,  0.0517, -0.0213],\n",
      "        [ 0.0425, -0.0072,  0.0066,  ...,  0.0113,  0.0168, -0.0580]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.4.attention.self.query.bias | Size: torch.Size([768]) | Values : tensor([ 0.0966, -0.0828]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.4.attention.self.key.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0409, -0.0425,  0.0406,  ...,  0.0011,  0.0408,  0.0235],\n",
      "        [-0.0086,  0.0788, -0.0403,  ...,  0.0017,  0.0205,  0.0079]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.4.attention.self.key.bias | Size: torch.Size([768]) | Values : tensor([-0.1203,  0.0561]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.4.attention.self.value.weight | Size: torch.Size([768, 768]) | Values : tensor([[-0.0021, -0.0797, -0.0051,  ...,  0.0189,  0.0074,  0.0325],\n",
      "        [ 0.0376,  0.0568,  0.0009,  ...,  0.0092,  0.0217,  0.0521]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.4.attention.self.value.bias | Size: torch.Size([768]) | Values : tensor([-0.0252,  0.0122]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.4.attention.output.dense.weight | Size: torch.Size([768, 768]) | Values : tensor([[-0.0571, -0.0128, -0.0041,  ...,  0.0143,  0.0072,  0.0622],\n",
      "        [ 0.0417, -0.0335, -0.0661,  ..., -0.1480,  0.1015, -0.0109]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.4.attention.output.dense.bias | Size: torch.Size([768]) | Values : tensor([-0.0085,  0.0150]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.4.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.5798, 0.2102]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.4.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([-0.1047, -0.1117]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.4.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Values : tensor([[-0.0157,  0.0776,  0.0542,  ..., -0.0158,  0.0352,  0.0309],\n",
      "        [ 0.0795, -0.0292,  0.1113,  ..., -0.0075,  0.1386, -0.0021]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.4.intermediate.dense.bias | Size: torch.Size([3072]) | Values : tensor([-0.1464, -0.1141]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.4.output.dense.weight | Size: torch.Size([768, 3072]) | Values : tensor([[ 0.0469,  0.0478,  0.0261,  ...,  0.0350,  0.0368, -0.0115],\n",
      "        [ 0.0261, -0.0603, -0.0421,  ...,  0.0144,  0.0738, -0.0108]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.4.output.dense.bias | Size: torch.Size([768]) | Values : tensor([-0.0456,  0.1237]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.4.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.4736, 0.2881]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.4.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([0.0129, 0.0178]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.5.attention.self.query.weight | Size: torch.Size([768, 768]) | Values : tensor([[-0.0095,  0.0070,  0.0381,  ...,  0.0312,  0.0426,  0.0162],\n",
      "        [ 0.0097,  0.0027,  0.0841,  ...,  0.0030,  0.0113, -0.0214]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.5.attention.self.query.bias | Size: torch.Size([768]) | Values : tensor([-0.0044,  0.0672]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.5.attention.self.key.weight | Size: torch.Size([768, 768]) | Values : tensor([[-0.0185,  0.0308,  0.0012,  ..., -0.0173,  0.0139,  0.0317],\n",
      "        [ 0.0022, -0.1220, -0.0128,  ...,  0.0466, -0.0176,  0.0151]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.5.attention.self.key.bias | Size: torch.Size([768]) | Values : tensor([0.0770, 0.1539]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.5.attention.self.value.weight | Size: torch.Size([768, 768]) | Values : tensor([[-0.0288,  0.0186, -0.0555,  ...,  0.0577,  0.0196,  0.0226],\n",
      "        [ 0.0797, -0.0011,  0.0470,  ...,  0.0013, -0.0236, -0.0248]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.5.attention.self.value.bias | Size: torch.Size([768]) | Values : tensor([ 0.0198, -0.0167]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.5.attention.output.dense.weight | Size: torch.Size([768, 768]) | Values : tensor([[-2.3177e-03, -4.3437e-02, -1.8076e-02,  ...,  3.3929e-03,\n",
      "         -3.1082e-02, -1.2498e-02],\n",
      "        [ 9.2157e-03,  8.7457e-02, -1.6950e-01,  ..., -5.5238e-05,\n",
      "         -4.0722e-02, -7.6366e-02]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.5.attention.output.dense.bias | Size: torch.Size([768]) | Values : tensor([-0.0245, -0.0536]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.5.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.6779, 0.2487]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.5.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([-0.0999, -0.1599]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.5.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Values : tensor([[ 0.0050,  0.1250,  0.0556,  ...,  0.0288,  0.0390,  0.0809],\n",
      "        [-0.0172,  0.0908,  0.0091,  ...,  0.0027,  0.0650,  0.0053]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.5.intermediate.dense.bias | Size: torch.Size([3072]) | Values : tensor([-0.3318, -0.0261]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.5.output.dense.weight | Size: torch.Size([768, 3072]) | Values : tensor([[ 0.0214, -0.0266, -0.0716,  ...,  0.0037,  0.0131, -0.0164],\n",
      "        [-0.0215,  0.0263, -0.0281,  ..., -0.0191, -0.0568,  0.0403]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.5.output.dense.bias | Size: torch.Size([768]) | Values : tensor([0.0089, 0.0556]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.5.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.4960, 0.3726]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.5.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([0.0192, 0.0170]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.6.attention.self.query.weight | Size: torch.Size([768, 768]) | Values : tensor([[-0.0316, -0.0330,  0.0094,  ...,  0.0275,  0.0040,  0.0715],\n",
      "        [ 0.1244,  0.0065,  0.0895,  ..., -0.0741,  0.0709, -0.0337]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.6.attention.self.query.bias | Size: torch.Size([768]) | Values : tensor([0.0548, 0.2529]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.6.attention.self.key.weight | Size: torch.Size([768, 768]) | Values : tensor([[-0.0047, -0.0146,  0.0037,  ...,  0.0174,  0.0397, -0.0229],\n",
      "        [ 0.0069,  0.0772,  0.0454,  ..., -0.0151, -0.0562,  0.0055]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.6.attention.self.key.bias | Size: torch.Size([768]) | Values : tensor([-0.0341,  0.0283]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.6.attention.self.value.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0131,  0.0670,  0.0101,  ...,  0.0018, -0.0241,  0.0075],\n",
      "        [-0.0397,  0.0614,  0.0906,  ...,  0.0250,  0.0186,  0.0185]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.6.attention.self.value.bias | Size: torch.Size([768]) | Values : tensor([0.1477, 0.0169]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.6.attention.output.dense.weight | Size: torch.Size([768, 768]) | Values : tensor([[-6.5750e-03, -4.2136e-03,  5.3315e-03,  ..., -5.0075e-03,\n",
      "          2.5738e-02,  4.0353e-03],\n",
      "        [ 8.8635e-02, -8.4007e-02, -6.1670e-05,  ...,  6.4442e-02,\n",
      "          5.2643e-02,  2.9849e-02]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.6.attention.output.dense.bias | Size: torch.Size([768]) | Values : tensor([-0.0604,  0.0348]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.6.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.6394, 0.3130]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.6.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([-0.1146, -0.1307]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.6.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Values : tensor([[ 0.0999,  0.0806,  0.0344,  ...,  0.0095,  0.0926,  0.0423],\n",
      "        [ 0.0015, -0.0187,  0.0064,  ..., -0.0114,  0.0024, -0.0195]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.6.intermediate.dense.bias | Size: torch.Size([3072]) | Values : tensor([-0.3009, -0.1017]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.6.output.dense.weight | Size: torch.Size([768, 3072]) | Values : tensor([[-0.0442, -0.0146, -0.0098,  ..., -0.0106, -0.0386, -0.0227],\n",
      "        [ 0.0068, -0.0341,  0.0535,  ..., -0.0530,  0.0848,  0.0939]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.6.output.dense.bias | Size: torch.Size([768]) | Values : tensor([0.0119, 0.0779]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.6.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.5096, 0.4136]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.6.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([-0.0090, -0.0044]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.7.attention.self.query.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0312,  0.0936,  0.0830,  ...,  0.0548, -0.0083,  0.0437],\n",
      "        [ 0.0072, -0.0662,  0.0605,  ...,  0.0167,  0.0471, -0.0630]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.7.attention.self.query.bias | Size: torch.Size([768]) | Values : tensor([-0.2003,  0.0481]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.7.attention.self.key.weight | Size: torch.Size([768, 768]) | Values : tensor([[-0.0079, -0.0397,  0.0031,  ...,  0.0215, -0.0840, -0.0109],\n",
      "        [ 0.0652, -0.0290,  0.0741,  ...,  0.0492,  0.0162,  0.0101]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.7.attention.self.key.bias | Size: torch.Size([768]) | Values : tensor([ 0.0045, -0.0447]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.7.attention.self.value.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0293, -0.0664, -0.0629,  ..., -0.0147, -0.0815, -0.0271],\n",
      "        [ 0.0482, -0.0801, -0.0159,  ..., -0.0182, -0.0127,  0.0085]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.7.attention.self.value.bias | Size: torch.Size([768]) | Values : tensor([-0.0046, -0.0039]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.7.attention.output.dense.weight | Size: torch.Size([768, 768]) | Values : tensor([[-0.0034,  0.0143,  0.0282,  ..., -0.0030,  0.0008, -0.0103],\n",
      "        [ 0.0095,  0.0179, -0.0042,  ..., -0.0125,  0.0091,  0.0322]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.7.attention.output.dense.bias | Size: torch.Size([768]) | Values : tensor([ 0.0232, -0.0627]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.7.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.6813, 0.3238]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.7.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([-0.1115, -0.1061]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.7.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Values : tensor([[ 0.0098,  0.0015, -0.0041,  ...,  0.0121,  0.0146, -0.0166],\n",
      "        [ 0.0396, -0.0100,  0.0804,  ..., -0.0594, -0.0503, -0.0025]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.7.intermediate.dense.bias | Size: torch.Size([3072]) | Values : tensor([-0.0315, -0.1273]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.7.output.dense.weight | Size: torch.Size([768, 3072]) | Values : tensor([[-0.0233,  0.0372,  0.1049,  ...,  0.0618,  0.0549, -0.0254],\n",
      "        [ 0.0662, -0.0483, -0.0043,  ..., -0.0407,  0.0330,  0.0317]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.7.output.dense.bias | Size: torch.Size([768]) | Values : tensor([-0.0423,  0.0446]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.7.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.5152, 0.3785]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.7.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([-0.0026,  0.0004]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.8.attention.self.query.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0261, -0.0255,  0.0665,  ...,  0.0917, -0.0430, -0.0392],\n",
      "        [-0.1089,  0.0319, -0.0023,  ..., -0.0016, -0.0214, -0.0111]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.8.attention.self.query.bias | Size: torch.Size([768]) | Values : tensor([-0.0085,  0.0214]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.8.attention.self.key.weight | Size: torch.Size([768, 768]) | Values : tensor([[-0.0496, -0.0872,  0.0683,  ...,  0.0179, -0.0269, -0.0783],\n",
      "        [-0.0363,  0.0620, -0.1266,  ..., -0.0739, -0.0308, -0.0197]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.8.attention.self.key.bias | Size: torch.Size([768]) | Values : tensor([ 0.0183, -0.0695]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.8.attention.self.value.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0343, -0.0638,  0.0190,  ..., -0.0097,  0.0102, -0.0260],\n",
      "        [-0.0062, -0.0527,  0.0106,  ...,  0.0843, -0.0081, -0.0253]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.8.attention.self.value.bias | Size: torch.Size([768]) | Values : tensor([-0.0050, -0.0003]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.8.attention.output.dense.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0678,  0.0137, -0.0178,  ...,  0.0423,  0.0191, -0.0206],\n",
      "        [ 0.0095, -0.0146,  0.0261,  ...,  0.0062, -0.0074,  0.0126]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.8.attention.output.dense.bias | Size: torch.Size([768]) | Values : tensor([-0.0051, -0.1208]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.8.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.5470, 0.3015]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.8.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([0.0178, 0.1326]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.8.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Values : tensor([[-0.0443,  0.0026, -0.0959,  ...,  0.0205, -0.0623, -0.0020],\n",
      "        [ 0.0183,  0.0097,  0.1374,  ..., -0.0536, -0.1306, -0.0230]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.8.intermediate.dense.bias | Size: torch.Size([3072]) | Values : tensor([-0.0489, -0.0632]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.8.output.dense.weight | Size: torch.Size([768, 3072]) | Values : tensor([[-0.0372, -0.0224, -0.0939,  ...,  0.0308, -0.0404,  0.0254],\n",
      "        [ 0.0580,  0.0369,  0.0452,  ..., -0.0262, -0.0456, -0.0230]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.8.output.dense.bias | Size: torch.Size([768]) | Values : tensor([ 0.0044, -0.0063]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.8.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.5106, 0.3184]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.8.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([ 0.0239, -0.0052]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.9.attention.self.query.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0203,  0.0052, -0.0112,  ...,  0.0265, -0.0018, -0.0312],\n",
      "        [ 0.0141,  0.0069,  0.0618,  ..., -0.0022,  0.1223, -0.0666]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.9.attention.self.query.bias | Size: torch.Size([768]) | Values : tensor([-0.2313,  0.1983]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.9.attention.self.key.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0561, -0.0007,  0.0413,  ..., -0.0044,  0.0147, -0.0787],\n",
      "        [-0.0004,  0.0165,  0.0803,  ...,  0.0722, -0.0543, -0.0721]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.9.attention.self.key.bias | Size: torch.Size([768]) | Values : tensor([0.0915, 0.0524]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.9.attention.self.value.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0302,  0.0298, -0.0479,  ...,  0.0250,  0.0517, -0.0008],\n",
      "        [-0.0165,  0.0535,  0.0424,  ..., -0.0683,  0.0210,  0.0293]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.9.attention.self.value.bias | Size: torch.Size([768]) | Values : tensor([ 0.0052, -0.0004]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.9.attention.output.dense.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0447,  0.0208, -0.0097,  ...,  0.0313,  0.0136, -0.0257],\n",
      "        [ 0.0447,  0.0477,  0.0791,  ..., -0.0099,  0.0091, -0.0082]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.9.attention.output.dense.bias | Size: torch.Size([768]) | Values : tensor([-0.0697, -0.0922]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.9.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.4641, 0.2327]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.9.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([0.0982, 0.1154]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.9.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Values : tensor([[-0.0299, -0.0140,  0.0041,  ...,  0.0804,  0.0454,  0.0304],\n",
      "        [-0.0191,  0.0065, -0.0405,  ...,  0.0765, -0.0235, -0.0287]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.9.intermediate.dense.bias | Size: torch.Size([3072]) | Values : tensor([-0.0757, -0.1579]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.9.output.dense.weight | Size: torch.Size([768, 3072]) | Values : tensor([[ 5.9745e-05,  1.5768e-02, -2.4801e-03,  ..., -2.7293e-02,\n",
      "          1.9720e-02, -6.1161e-02],\n",
      "        [ 7.1894e-03,  1.0848e-02,  8.4973e-02,  ...,  4.6867e-02,\n",
      "          9.8572e-03,  4.9592e-02]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.9.output.dense.bias | Size: torch.Size([768]) | Values : tensor([ 0.0515, -0.0052]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.9.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.5618, 0.3770]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.9.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([0.0183, 0.0270]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.10.attention.self.query.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0137, -0.0104, -0.1269,  ..., -0.0387,  0.0665, -0.1543],\n",
      "        [-0.0307,  0.0074, -0.0412,  ..., -0.0092, -0.0842, -0.0462]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.10.attention.self.query.bias | Size: torch.Size([768]) | Values : tensor([-0.0572,  0.0510]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.10.attention.self.key.weight | Size: torch.Size([768, 768]) | Values : tensor([[-0.0346, -0.0912, -0.0119,  ..., -0.1117, -0.0449, -0.0979],\n",
      "        [-0.0238,  0.0434, -0.0336,  ..., -0.0011, -0.0329,  0.0527]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.10.attention.self.key.bias | Size: torch.Size([768]) | Values : tensor([ 0.0831, -0.0105]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.10.attention.self.value.weight | Size: torch.Size([768, 768]) | Values : tensor([[-0.0076,  0.0136,  0.0983,  ..., -0.0293, -0.0241,  0.0335],\n",
      "        [ 0.0318, -0.0167, -0.0238,  ...,  0.0247, -0.0290, -0.0053]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.10.attention.self.value.bias | Size: torch.Size([768]) | Values : tensor([ 0.0003, -0.0124]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.10.attention.output.dense.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0612, -0.0189, -0.0193,  ..., -0.0155, -0.0101, -0.0015],\n",
      "        [ 0.0211, -0.0022, -0.0236,  ..., -0.0265,  0.0124,  0.0526]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.10.attention.output.dense.bias | Size: torch.Size([768]) | Values : tensor([ 0.0021, -0.1393]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.10.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.5135, 0.4067]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.10.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([0.1132, 0.1061]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.10.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Values : tensor([[-0.1043, -0.0225,  0.0642,  ..., -0.0109,  0.0134,  0.0081],\n",
      "        [ 0.0645, -0.0370,  0.0679,  ..., -0.0516, -0.0238,  0.0044]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.10.intermediate.dense.bias | Size: torch.Size([3072]) | Values : tensor([-0.0970, -0.0681]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.10.output.dense.weight | Size: torch.Size([768, 3072]) | Values : tensor([[ 4.7491e-02, -1.4423e-02,  3.7756e-02,  ..., -4.1547e-02,\n",
      "          1.1522e-04, -2.9797e-02],\n",
      "        [-3.5799e-02, -1.1659e-01,  4.9668e-02,  ...,  6.9096e-02,\n",
      "          3.5584e-02, -1.7387e-02]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.10.output.dense.bias | Size: torch.Size([768]) | Values : tensor([0.0462, 0.0181]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.10.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.4797, 0.4331]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.10.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([-0.0075,  0.0202]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.11.attention.self.query.weight | Size: torch.Size([768, 768]) | Values : tensor([[-0.0838, -0.0215,  0.0168,  ...,  0.0074,  0.0371,  0.0177],\n",
      "        [-0.0103, -0.0299,  0.0115,  ...,  0.0147, -0.0083, -0.0060]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.11.attention.self.query.bias | Size: torch.Size([768]) | Values : tensor([ 0.1109, -0.1379]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.11.attention.self.key.weight | Size: torch.Size([768, 768]) | Values : tensor([[-0.0654,  0.0347, -0.0190,  ...,  0.0188,  0.0066, -0.0023],\n",
      "        [-0.0323,  0.1069,  0.0437,  ...,  0.0096,  0.0783, -0.0344]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.11.attention.self.key.bias | Size: torch.Size([768]) | Values : tensor([0.0032, 0.1855]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.11.attention.self.value.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0083, -0.0579,  0.1359,  ...,  0.0186, -0.0293, -0.0126],\n",
      "        [-0.0018,  0.0377, -0.0119,  ...,  0.0199,  0.0003, -0.0374]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.11.attention.self.value.bias | Size: torch.Size([768]) | Values : tensor([ 0.0043, -0.0158]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.11.attention.output.dense.weight | Size: torch.Size([768, 768]) | Values : tensor([[-0.0210, -0.0396,  0.0531,  ..., -0.0225, -0.0362,  0.0192],\n",
      "        [-0.0026, -0.0626,  0.0202,  ..., -0.0275, -0.0182,  0.0012]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.11.attention.output.dense.bias | Size: torch.Size([768]) | Values : tensor([-0.0048,  0.0122]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.11.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.4746, 0.4251]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.11.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([0.1033, 0.0777]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.11.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Values : tensor([[-0.0085, -0.0086,  0.0116,  ..., -0.0355, -0.0110, -0.0180],\n",
      "        [ 0.0079, -0.0474, -0.0496,  ..., -0.0197,  0.0381, -0.0191]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.11.intermediate.dense.bias | Size: torch.Size([3072]) | Values : tensor([-0.0266, -0.0273]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.11.output.dense.weight | Size: torch.Size([768, 3072]) | Values : tensor([[-0.0296, -0.0187, -0.0396,  ..., -0.0158,  0.0709,  0.0453],\n",
      "        [ 0.0517, -0.0274, -0.0392,  ...,  0.0347, -0.0822, -0.0752]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.11.output.dense.bias | Size: torch.Size([768]) | Values : tensor([ 0.0510, -0.0688]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.11.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.3291, 0.3815]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.layer.11.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([-0.0702,  0.0021]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.fpn1.0.weight | Size: torch.Size([768, 768, 2, 2]) | Values : tensor([[[[ 4.8308e-02,  2.2656e-02],\n",
      "          [ 2.3469e-02,  2.0679e-02]],\n",
      "\n",
      "         [[-5.8407e-03,  1.5653e-02],\n",
      "          [ 1.0187e-02, -3.3416e-02]],\n",
      "\n",
      "         [[ 3.7170e-04,  1.8004e-02],\n",
      "          [ 2.7276e-02, -4.2429e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.9548e-02, -3.0766e-02],\n",
      "          [ 1.0501e-03, -1.9872e-02]],\n",
      "\n",
      "         [[ 6.5676e-03,  6.8612e-03],\n",
      "          [ 3.1141e-02,  1.6255e-02]],\n",
      "\n",
      "         [[ 9.7302e-03,  2.0815e-02],\n",
      "          [ 2.7607e-02,  1.7032e-02]]],\n",
      "\n",
      "\n",
      "        [[[-2.2675e-02, -1.1858e-02],\n",
      "          [-1.4024e-02, -6.2322e-04]],\n",
      "\n",
      "         [[-2.6706e-02, -5.0474e-02],\n",
      "          [-1.6688e-02, -1.2569e-02]],\n",
      "\n",
      "         [[-9.7653e-03, -5.9958e-05],\n",
      "          [ 3.2599e-03, -2.9196e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.4123e-02,  2.0965e-03],\n",
      "          [-3.0027e-02,  5.8726e-03]],\n",
      "\n",
      "         [[-9.0746e-03,  3.6181e-02],\n",
      "          [-1.1276e-02,  1.6729e-02]],\n",
      "\n",
      "         [[-5.5891e-02, -2.9636e-02],\n",
      "          [ 1.4114e-02,  2.0810e-02]]]]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.fpn1.0.bias | Size: torch.Size([768]) | Values : tensor([-0.0072, -0.0062]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.fpn1.1.weight | Size: torch.Size([768]) | Values : tensor([0.7250, 0.7019]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.fpn1.1.bias | Size: torch.Size([768]) | Values : tensor([-0.0794, -0.0698]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.fpn1.1.running_mean | Size: torch.Size([768]) | Values : tensor([0.4449, 0.3220]) \n",
      "\n",
      "Layer: backbone.bottom_up.backbone.encoder.fpn1.1.running_var | Size: torch.Size([768]) | Values : tensor([0.2901, 0.2883]) \n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "slice() cannot be applied to a 0-dim tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m model_layers \u001b[38;5;241m=\u001b[39m model_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_name, params \u001b[38;5;129;01min\u001b[39;00m model_layers\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Values : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: slice() cannot be applied to a 0-dim tensor."
     ]
    }
   ],
   "source": [
    "# Kiểm tra chi tiết về thành phần 'model'\n",
    "model_data = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "\n",
    "# Nếu model là một OrderedDict, bạn có thể duyệt qua các lớp trong mô hình\n",
    "model_layers = model_data['model']\n",
    "\n",
    "for layer_name, params in model_layers.items():\n",
    "    print(f\"Layer: {layer_name} | Size: {params.size()} | Values : {params[:2]} \\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
